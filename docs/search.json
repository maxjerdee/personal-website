[
  {
    "objectID": "papers.html",
    "href": "papers.html",
    "title": "Papers",
    "section": "",
    "text": "M. Jerdee, A. Kirkley, and M. E. J. Newman, Normalized mutual information is a biased measure for classification and community detection. Preprint arXiv:2307.01282 (2024)."
  },
  {
    "objectID": "papers.html#pre-prints",
    "href": "papers.html#pre-prints",
    "title": "Papers",
    "section": "",
    "text": "M. Jerdee, A. Kirkley, and M. E. J. Newman, Normalized mutual information is a biased measure for classification and community detection. Preprint arXiv:2307.01282 (2024)."
  },
  {
    "objectID": "papers.html#peer-reviewed-journal-publications",
    "href": "papers.html#peer-reviewed-journal-publications",
    "title": "Papers",
    "section": "Peer reviewed journal publications",
    "text": "Peer reviewed journal publications\n\nM. Jerdee, A. Kirkley, and M. E. J. Newman, Mutual information and the encoding of contingency tables. in press, Physical Review E. Preprint arXiv:2405.05393 (2024).\nM. Jerdee and M. E. J. Newman, Luck, skill, and depth of competition in games and social hierarchies. in press, Science Advances. Preprint arXiv:2312.04711 (2024). [code] [video]\nM. Jerdee, A. Kirkley, and M. E. J. Newman, Improved estimates for the number of non-negative integer matrices with given row and column sums. Proc. R. Soc. London A 480, 20230470 (2024). [code]\nA. G. Lezcano, M. Jerdee, and L. A. P. Zayas, Cardy expansion of 3d superconformal indices and corrections to the dual black hole entropy. Journal of High Energy Physics 1, 1–46 (2023).\nD. J. Binder, S. M. Chester, and M. Jerdee, ABJ correlators with weakly broken higher spin symmetry. Journal of High Energy Physics 4, 1–57 (2021).\nD. J. Binder, S. M. Chester, M. Jerdee, and S. S. Pufu, The 3d \\(\\mathcal{N} = 6\\) bootstrap: from higher spins to strings to membranes. Journal of High Energy Physics 5, 1–63 (2021).\nP. Melchior, F. Moolekamp, M. Jerdee, R. Armstrong, A. L. Sun, J. Bosch, and R. Lupton, SCARLET: Source separation in multi-band images by constrained matrix factorization. Astronomy and Computing 24, 129–142 (2018)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Max(imilian) Jerdee",
    "section": "",
    "text": "I am a Physics PhD student at the University of Michigan advised by Prof. Mark Newman and affiliated with the Center for the Study of Complex Systems. I will be starting as an Omidyar Postdoctoral Fellow at the Santa Fe Institute in Fall 2025.\nI enjoy researching topics between physics, networks, and statistics (more info here) and building somewhat educational tools and games (check them out)."
  },
  {
    "objectID": "demos/wikiguess.html",
    "href": "demos/wikiguess.html",
    "title": "WikiGuess",
    "section": "",
    "text": "An estimation game where you can guess numbers from all sorts of wikipedia excerpts. You can also play with friends online by creating a room and guessing on the same questions. In making it, there are some interesting questions about what makes a number fun to guess.\nPlay here (the multiplayer is still a bit buggy)"
  },
  {
    "objectID": "demos/parallel_tempering.html",
    "href": "demos/parallel_tempering.html",
    "title": "Parallel tempering",
    "section": "",
    "text": "Monte Carlo algorithms are often used to explore probability distributions. Particularly, the Metropolis-Hastings algorithm is an extremely widely used technique for sampling from distributions. In this story, there are a few frog characters to familiarize ourselves with.\nFirst up, explorer frog:\n\nDoesn’t give a flip\nRed\n\nNext, we have exploiter frog (also known as greedy frog):\n\nWants flies, now\nBlue\n\nFinally, our most complicated frog will be sampler frog:\n\nWants flies, most of the time (!)\nGreen\n\nIn the Metropolis-Hastings algorithm, in order to sample from a distribution proportional to \\(P(x)\\), a proposed move from a “state” \\(x\\) to some new state \\(x'\\) will be “accepted” with probability \\[P_{\\text{accept}}(x \\rightarrow x') = \\min\\left(1,\\frac{P(x)}{P(x')}\\right).\\] (really we should be sure our proposals are symmetric, but we won’t worry about that in this demonstration).\nWhat is going on with our frogs is that there is a\nSo, generally by using parallel tempering method we can quite generically improve the convergence performance of Monte Carlo methods, and therefore the efficiency of our posterior inference.\nA nice side-effect of this, however, is the ability to leverage a technique known as “calorimetry” in the Physics community for computing the free energy of a system, and as “thermodynamic integration” within the statistis community.\nSuppose that we have a general model where the\nThe fundamental identity that will come in handy is that if we define the partition function at an arbitrary \\(\\beta\\) as\n\\[ Z(\\beta) = \\int P(A|\\vec{\\theta})P(\\vec{\\theta})^\\beta d \\theta\\]\nso that notably we have that the Bayesian evidence is obtained at \\(\\beta = 1\\):\n\\[ Z(1) = \\int P(A | \\vec{\\theta}) P(\\vec{\\theta}) d \\theta = P(A), \\qquad Z(0) = \\int P(\\theta) d\\theta = 1\\]\nKey terms/concepts:\n\nSimulated annealing is generally a useful way to improve convergence."
  },
  {
    "objectID": "demos/metropolis_hastings.html",
    "href": "demos/metropolis_hastings.html",
    "title": "MCMC",
    "section": "",
    "text": "This demo is based upon Figure 12.1 from John Miller’s book A Crude Look at the Whole.\nMonte Carlo algorithms are often used to explore probability distributions. Particularly, the Metropolis-Hastings algorithm is an extremely widely used technique for sampling from distributions. In this story, there are a few frog characters to familiarize ourselves with.\nFirst up, explorer frog:\n\nRed\nDoesn’t care about flies, wanders to any lilypad.\n\nNext, we have exploiter frog (also known as greedy frog):\n\nBlue\nWants flies, now.\n\nFinally, our most complicated frog will be sampler frog:\n\nGreen\nWants flies, most of the time (!)\n\nIn the Metropolis-Hastings algorithm, in order to sample from a distribution proportional to \\(P(x)\\), a proposed move from a “state” \\(x\\) to some new state \\(x'\\) will be “accepted” with probability \\[P_{\\text{accept}}(x \\rightarrow x') = \\min\\left(1,\\frac{P(x)}{P(x')}\\right).\\] (really we should be sure our proposals are symmetric, but we won’t worry about that in this demonstration).\nIn this analogy, the states \\(x\\) are the different lilypads, and the probability \\(P(x)\\) is the number of flies at each lilypad. The frog characters then correspond to the behavior of this Monte Carlo algorithm at different values of \\(\\beta\\). Below, you can tinker with these parameters to explore how the distribution of frogs is related to the distribution of flies depending on the \\(\\beta\\) parameter."
  },
  {
    "objectID": "demos/chaos_sampler.html",
    "href": "demos/chaos_sampler.html",
    "title": "Chaos sampler",
    "section": "",
    "text": "Play Sound\n\n \nChaotic systems are everywhere in nature. These systems are characterized by the way that small changes can snowball into massive changes in behavior, rendering them hard to predict. For example, in our planet’s climate the flap of a butterfly wing in the right place and right time can cause a hurricane to form. In this interactive demo we leverage the unpredictability of a simple chaotic system known as the Lorentz attractor to remix music recordings in unpredictable ways. Particularly, we split a recording (or “sample”) of music into evenly spaced chunks. A trajectory of the Lorentz attractor is then used to associate each of these pieces of the recording to a point in the system’s phase space. The music is then shuffled by continuing to run the trajectory and playing the chunk associated with the nearest written point in phase space at each step. By harnessing this chaos as a creative force we can play with this ubiquitous feature of the world in an accessible way.\nThis demo made during the 2024 SFI Complex Systems Summer School as part of the Art and Complexity Collective, and was inspired by Liz Bradley’s talks on Nonlinear Dynamics. Particularly, heavy inspiration taken from Diana Dabby’s work (paper, video) on chaotic variations using the Lorentz attractor."
  },
  {
    "objectID": "demos/game_of_life.html",
    "href": "demos/game_of_life.html",
    "title": "Game of life",
    "section": "",
    "text": "Conway’s game of life is a compact, elegant example of how simple principles can generate complex behaviors. The “game” is played on a 2-dimensional grid, and loosely simulates changes in the population of a species over space and time. Each grid square may either be “alive” or “dead” at each time interval. Only if a grid cell has between 3 and 5 of its 8 neighbors alive will it be alive and happy on the next turn, otherwise it will die of either under- or over-crowding. From this simple rule alone, very complicated behavior can be observed. In this demo we simulate this game of life and play sounds when each of the grid cells is alive. We also mirror our simulation on a Novation Launchpad, an 8 x 8 grid of buttons that can light up with colors. We hope that by creating a unified physical, auditory, and visual experience we have an engaging mode to play with this toy example of how complexity can emerge.\nThis demo made during the 2024 SFI Complex Systems Summer School as part of the Art and Complexity Collective.\nIf you plug in a Novation Mini [MK3], you can interact with this demonstration live. That is also what the request to “reprogram your MIDI devices” is about, otherwise you can select “Block” and simply click on the buttons within the browser.\n\n\n\nExample of asking for permissions, unless you are plugging in a Launchpad can select to Block"
  },
  {
    "objectID": "demos/metropolis_hastings_flat.html",
    "href": "demos/metropolis_hastings_flat.html",
    "title": "MCMC Flat",
    "section": "",
    "text": "Monte Carlo algorithms are often used to explore probability distributions. Particularly, the Metropolis-Hastings algorithm is an extremely widely used technique for sampling from distributions. In this story, there are a few frog characters to familiarize ourselves with.\nFirst up, explorer frog:\n\nDoesn’t give a flip\nRed\nHot\n\nNext, we have exploiter frog (also known as greedy frog):\n\nWants flies, now\nBlue\nCold\n\nFinally, our most complicated frog will be sampler frog:\n\nWants flies, most of the time (!)\nGreen\nJust right\n\nIn the Metropolis-Hastings algorithm, in order to sample from a distribution proportional to \\(P(x)\\), a proposed move from a “state” \\(x\\) to some new state \\(x'\\) will be “accepted” with probability \\[P_{\\text{accept}}(x \\rightarrow x') = \\min\\left(1,\\frac{P(x)}{P(x')}\\right).\\] (really we should be sure our proposals are symmetric, but we won’t worry about that in this demonstration).\nWhat is going on with our frogs is that there is a\nSo, generally by using parallel tempering method we can quite generically improve the convergence performance of Monte Carlo methods, and therefore the efficiency of our posterior inference.\nA nice side-effect of this, however, is the ability to leverage a technique known as “calorimetry” in the Physics community for computing the free energy of a system, and as “thermodynamic integration” within the statistis community.\nSuppose that we have a general model where the\nThe fundamental identity that will come in handy is that if we define the partition function at an arbitrary \\(\\beta\\) as\n\\[ Z(\\beta) = \\int P(A|\\vec{\\theta})^\\beta P(\\vec{\\theta}) d \\theta\\]\nso that notably we have that the Bayesian evidence is obtained at \\(\\beta = 1\\):\n\\[\n\\begin{align*}\nZ(1) &= \\int P(A | \\vec{\\theta}) P(\\vec{\\theta}) d \\theta = P(A), \\\\\nZ(0) &= \\int P(\\theta) d\\theta = 1\n\\end{align*}\n\\]\nTODO:\n\nSwitch the frogs to a raster render to improve efficiency Implement as an svg with a background color that can be changed along with a png on top that handles the hard outline and eyes, and the blush and shading as opacity layers.\nMake an annealing option that cools down all of the frogs at once (implement as an annealing speed slider)\nReset accumulation histogram when changing beta\nSnap the inverse temperature slier around beta = 1 (unless annealing)\nCreate an asset of spins arranging in order to explain the temperature and tempering idea\nAdd left/right buttons in the top left in order to flip through examples (disable these in the notes post) [Add the ability to highlight these buttons]\nExamples (include a list in the demo page, designed to be written in a more technical way to describe what is going on likely for those who have already seen this stuff before. Link to the notes post for a more pedagogical explanation.)\n\nSingle red frog [triangle]\nMany red frogs, high simulation speed\nSingle blue frog\nMany blue frogs, high simulation speed\nSingle green frog\nMany green frogs, high simulation speed\nMany red frogs, high speed [line]\nBlue frogs, high speed\nGreen frogs, high speed (introduce the idea of mixing time)\nSimulated annealing, medium speed (start on red, cool down)\nSimulated annealing, fast speed"
  },
  {
    "objectID": "demos/relativity.html",
    "href": "demos/relativity.html",
    "title": "Special relativity",
    "section": "",
    "text": "When you move faster, your personal (relative!) time slows down. This is one of the many suprising consequences of the theory of relativity, that space and time are intwined into a single spacetime and that the actions we take in one dimension (moving through space) can impact our trajectory through the other (moving through time). This demo uses your phone’s accelerometer data in order to estimate your velocity, and so estimate how much your time slows down as you walk around.\nThis demo was built as part of the Fall 2021 Science Communication Fellows program at the University of Michigan Museum of Natural History."
  },
  {
    "objectID": "demos.html",
    "href": "demos.html",
    "title": "Demos",
    "section": "",
    "text": "Assorted projects I have worked on, generally try to explore a neat idea in an interactive way.\n\n\n\n\n\n\n\n\n\n\n\n\nChaos sampler\n\n\nUse a chaotic system to chop up beat samples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGame of life\n\n\nMake music with Conway’s game of life\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMCMC\n\n\nExplore Monte Carlo methods with hopping frogs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMCMC Flat\n\n\nExplore Monte Carlo methods with hopping frogs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParallel tempering\n\n\nExplore Monte Carlo methods with hopping frogs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpecial relativity\n\n\nUse your phone to observe how much your time slows down as you move around\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWikiGuess\n\n\nCompete with friends to estimate numbers from Wikipedia\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes/2025/statistical_inference.html",
    "href": "notes/2025/statistical_inference.html",
    "title": "Statistical inference review",
    "section": "",
    "text": "In this note we review the basics of statistical inference using a simple toy model of flipping a coin. The ideas we review, like Bayesian inference and model selection, are widely used in network science to understand more complex systems. The presentation here is adapted from my PhD thesis."
  },
  {
    "objectID": "notes/2025/statistical_inference.html#a-fair-coin",
    "href": "notes/2025/statistical_inference.html#a-fair-coin",
    "title": "Statistical inference review",
    "section": "A fair coin",
    "text": "A fair coin\nSuppose we perform a simple experiment and record whether a two-sided coin lands \"heads\" or \"tails\" over \\(n\\) flips. Before the experiment begins we might hypothesize that we hold a fair coin: that all coin flips are independent and result in heads with probability \\(p = 0.5\\) and tails with probability \\(1 - p = 0.5\\). This natural assumption defines a model by assigning a likelihood of observing any particular sequence of heads and tails over the \\(n\\) trials \\[\\begin{aligned}\n    P(\\overbrace{\\text{H},...,\\text{T}}^{n \\text{ flips}}|p = 0.5) = \\frac{1}{2^n}. \\label{eq:coin-flip-fair-likelihood}\\end{aligned}\\]\nAlthough all possible sequences of \\(n\\) outcomes are equally likely to appear under this model, certain observations may lead us to doubt our initial hypothesis. Suppose every coin flip we observe lands on heads. Although a single heads does not raise suspicion, 10 heads in a row start to be quite surprising at a probability of \\(\\sim 0.1\\%\\). And after 100 fair heads in a row we may start to question our place in the universe. The question arises: how many heads should we observe before abandoning our initial conjecture? And if the coin is not fair, what is its nature? Statistics provides many frameworks for approaching these questions.\nIn the frequentist approach our initial assumption of fairness is called a null model (or \"null hypothesis\") which our experiment may then \"reject.\" In this picture we define a test statistic that captures some surprising aspect of our observed data, in this case the unusually large number of heads \\(n_H = n\\) observed. We then compute the likelihood that the null model could produce a result with a test statistic at least as extreme as that observed, known as the p-value. If this p-value is small it is unlikely the null model alone could generate an observation similar to ours. This improbability suggests our model is lacking and serves as grounds to \"reject\" it in favor of some alternative model more likely to have produced our observation.\nIn linear regression, these p-values are often reported to reject the possibility of a slope of 0, of no relation between two variables. In network science, simple null models are likewise used to demonstrate that some observed structural feature like community or hierarchy requires a richer model to reproduce, such as those discussed in Section [sec:random-graph-models].\nFor a fair coin the probability of observing \\(n_H\\) heads out of \\(n\\) trials is given by the binomial distribution \\[\\begin{aligned}\n    P(n_H|p = 0.5) = \\frac{1}{2^n} \\binom{n}{n_H}.\\end{aligned}\\] The p-value, the chance of observing some number of heads \\(n_H'\\) greater than or equal to \\(n_H\\), is then \\[\\begin{aligned}\n    P(n_H' \\geq n_H|p = 0.5) = \\frac{1}{2^n} \\sum_{n_H' \\geq n_H} \\binom{n}{n_H'}.\\end{aligned}\\] As shown in Figure 1, over 10 fair coin flips we expect an average of 5 heads in this model although small fluctuations such as 4 or 6 heads are also common. More lopsided outcomes are increasingly unlikely, so that the p-values of observing larger \\(n_H\\) approach zero.\n\n\n\nDistribution of the number of heads \\(n_H\\) observed over  \\(n = 10\\) flips of a fair coin and the resulting p-value probabilities of observing at least \\(n_H\\) heads. The region where the p-value is less than 0.05 is plotted in gray, a threshold to reject the null hypothesis of a fair coin.\n\n\nA smaller p-value yields a more statistically significant rejection of the null hypothesis. A threshold of \\(P &lt; 0.05\\) is often used as a minimum standard, in which case we would reject the hypothesis of a fair coin in our experiment if 9 or 10 of the coin flips land heads. This demarcation at \\(0.05\\) is arbitrary; however small there is always some chance that a genuine fair coin produced an observation as unusual as the one made. We would expect to obtain a p-value less than 0.05 and so reject the model in 1 of 20 experiments conducted with even a truly fair coin. The choice of p-value threshold reflects a tolerance for this possibility that our rejection is the result of chance rather than a genuine signal."
  },
  {
    "objectID": "notes/2025/statistical_inference.html#a-biased-coin",
    "href": "notes/2025/statistical_inference.html#a-biased-coin",
    "title": "Statistical inference review",
    "section": "A biased coin",
    "text": "A biased coin\nIf such significance testing has led us to doubt that the coin is fair, we may consider an alternative hypothesis. We could instead model a biased coin whose flips are still independent but land heads with some fixed probability \\(p \\in [0,1]\\) and tails with probability \\(1 - p\\). Under this assumption the likelihood of a sequence with \\(n_\\text{H}\\) heads and \\(n_\\text{T} = n - n_\\text{H}\\) tails is \\[\\begin{aligned}\nP(\\overbrace{\\text{H},...,\\text{T}}^{n_\\text{H} \\text{ heads}}|p) = p^{n_\\text{H}}(1-p)^{n - n_\\text{H}}. \\label{eq:coin-flip-likelihood}\\end{aligned}\\] This defines a nested model by generalizing the fair coin as the special case \\(p = 0.5\\). Thus the original fair coin model can be directly compared against other choices of the parameter \\(p\\) which each represent possible coins of varying levels of bias towards landing heads or tails. Before the experiment begins we imagine all these coins are possible and aim to infer the \"true\" value of \\(p\\) realized by our coin based on the observations.\nTo apply this model suppose that after 10 trials we observe a sequence \\[\\begin{aligned}\n    (\\text{H}, \\text{H}, \\text{T}, \\text{T}, \\text{H}, \\text{T}, \\text{T}, \\text{H}, \\text{T}, \\text{T}), \\label{eq:coin-flip-observations}\\end{aligned}\\] now a mixture of \\(n_\\text{H} = 4\\) heads and \\(n_{\\text{T}} = 6\\) tails. We fit the model likelihood Eq. [eq:coin-flip-likelihood] to this data by finding the value of \\(p\\) which would maximize the probability that our observed sequence occurred. For this model this is simply equal to the observed fraction of heads \\[\\begin{aligned}\n    \\hat{p}_{\\text{ML}} = \\frac{n_\\text{H}}{n}\\end{aligned}\\] where the \"ML\" subscript indicates that this is the maximum likelihood estimate of \\(p\\). Our sequence of observations [eq:coin-flip-observations] gives the estimate \\(\\hat{p}_{\\text{ML}} = 0.4\\).\nAlthough this \\(\\hat{p}_{\\text{ML}}\\) is the single parameter value most likely to have generated our observations, it is unclear how seriously we should take the estimate. If the coin was in fact fair, \\(p = 0.5\\), the slight imbalance towards tails we observe could very well be a random fluctuation in our small sample size as seen in Figure 1. In our earlier language the p-value is not small enough to warrant rejecting the null hypothesis of a fair coin. To conclude that the one \"true\" value of \\(p\\) is 0.4, for example to predict that 400 of the next 1000 flips will be heads, would likely overfit the data. As a more extreme example if we only observe a single coin flip which happens to land heads, the maximum likelihood estimate would conclude that \\(\\hat{p}_{\\text{ML}} = 1\\) and thus that the coin will always land heads. There should be uncertainty in our conclusions, particularly when they are based on such little evidence."
  },
  {
    "objectID": "notes/2025/statistical_inference.html#bayesian-inference",
    "href": "notes/2025/statistical_inference.html#bayesian-inference",
    "title": "Statistical inference review",
    "section": "Bayesian inference",
    "text": "Bayesian inference\nBy adopting a Bayesian perspective we can naturally represent this uncertainty in our inferences. In this framework we critically never exclude the possibility of any potential parameter value. Rather we infer from the data that some parameter values are more probable than others. At each stage of the experiment we represent our belief of likely values as a distribution over all possibilities rather than a single best-fit point.\nBefore considering the data we define a prior distribution (or \"prior\") over the parameters that reflects our initial assumptions. Depending on the context of our experiment we may have quite different expectations that will rightfully influence the conclusions we draw. For example if we use a standard legal tender coin we may hesitate to conclude that the coin is notably biased, even after observing 10 heads in a row. However, if the flips instead involve either a hypothetical or peculiar-looking coin there may be more room for doubt. The form of the prior distribution precisely specifies our initial assumptions.\nFor our example we will adopt a prior assumption that the coin is likely to be fair or approximately fair. Although a physical coin could conceivably be as biased as \\(p = 0.4\\), we expect a balanced \\(p = 0.5\\) to be far more likely. To represent this we define a prior distribution over possible \\(p\\) peaked at \\(p = 0.5\\), particularly a beta distribution \\(p \\sim B(20,20)\\) \\[\\begin{aligned}\n    P(p) = \\frac{41!}{20!20!} p^{20}(1-p)^{20}, \\label{eq:coin-flip-P-p}\\end{aligned}\\] plotted in Figure 2c. This particular form of the prior and the number 20 are arbitrarily chosen for this example, although they represent a reasonable assumption for this scenario.\n\n\n\n(a) Posterior distributions of the parameter \\(p\\) after observing 0 coin flips (a.k.a. prior distribution), 10, and 100 coin flips. In each experiment 40% of trials are heads, a ratio marked by the vertical line in each plot. The posterior distributions are proportional to the product of the likelihood (b) and prior (c) over \\(p\\). As more observations are made the posterior is increasingly informed by the model likelihood.\n\n\nAs we observe coin flips we update these prior beliefs to reflect new data and arrive at a new posterior distribution of likely parameter values. Using Bayes’ law this posterior distribution is proportional to the product of the model likelihood and the initial prior distribution as \\[\\begin{aligned}\n    P(p|\\text{H},...,\\text{T}) \\propto P(\\text{H},...,\\text{T}|p)P(p). \\label{eq:bayes-law-prop}\\end{aligned}\\] This form ensures that when no data is observed the posterior distribution is equal to the prior assumption \\(P(p)\\). As flips are recorded the influence of the prior distribution wanes as the posterior is dominated by the model likelihood \\(P(\\text{H},...,\\text{T}|p)\\). Figure 2 demonstrates this shift in the posterior distribution (a) from the prior (c) to the likelihood (b) after making 0, 10, and 100 observations at a ratio of 40% heads. As more observations are made with an empirical probability of \\(p = 0.4\\), our posterior distribution becomes increasingly concentrated around that belief. We must however observe sufficient evidence to overcome the strength of our initial assumptions before coming to that conclusion with certainty.\nThe width and form of the posterior distribution gives a full picture of the uncertainty of our the inference. Although the posterior distribution in Figure 2a after observing 100 flips is tightly concentrated around a probability \\(p \\sim 0.4\\), we retain some ambiguity if the parameter might deviate slightly from that peak. Nonetheless it is often useful to report the single choice of parameter most favored by the posterior distribution, known as the maximum a posteriori (MAP) estimate. For our example and choice of prior Eq. [eq:coin-flip-P-p] this is \\[\\begin{aligned}\n    \\hat{p}_{\\text{MAP}} = \\frac{n_H + 20}{n + 40}. \\label{eq:coin-flip-p-MAP}\\end{aligned}\\] The earlier interplay between the prior and likelihood is present in this estimate. When no coin flips are observed we obtain the prior assumption \\(\\hat{p}_{\\text{MAP}} = 0.5\\). As the number of observations \\(n\\) increases, this Bayesian estimate approaches the frequentist maximum likelihood estimate, \\(\\hat{p}_{\\text{MAP}} \\rightarrow \\frac{n_H}{n} = \\hat{p}_{\\text{ML}}\\)."
  },
  {
    "objectID": "notes/2025/statistical_inference.html#bayesian-evidence",
    "href": "notes/2025/statistical_inference.html#bayesian-evidence",
    "title": "Statistical inference review",
    "section": "Bayesian evidence",
    "text": "Bayesian evidence\nTo complete Bayes’ law and fully specify the posterior distribution we normalize Eq. [eq:bayes-law-prop] as \\[\\begin{aligned}\n    P(p|\\text{H},...,\\text{T}) &= \\frac{P(\\text{H},...,\\text{T}|p)P(p)}{P(\\text{H},...,\\text{T})} \\label{eq:bayes-law} \\end{aligned}\\] where \\[\\begin{aligned}\n    P(\\text{H},...,\\text{T}) = \\int_0^1 P(\\text{H},...,\\text{T}|p)P(p) dp\\end{aligned}\\] is the evidence (or \"marginal likelihood\") of the model. This evidence can be interpreted as the probability of arriving at the observations \\(\\text{H},...,\\text{T}\\) through the two stage process of first choosing a parameter \\(p\\) from the prior \\(P(p)\\) then generating the data from the model likelihood \\(P(\\text{H},...,\\text{T}|p)\\). By integrating over all possible parameters \\(p\\), the Bayesian evidence is equal to the total probability that a model generates the observed data from our prior assumptions. A cartoon of this generative process is given in Figure 3.\n\n\n\nSchematic of the full generative process of the fair and biased coin models. A parameter value \\(p\\) is first drawn from the appropriate prior \\(P(p)\\), represented by the thicknesses of the top gray arrows to example parameter values \\(p = 0.4, 0.5,\\) and \\(0.6\\). The data is drawn from the model likelihood \\(P(\\text{H},...,\\text{T}|p)\\) given that parameter. Examples of the likelihood of three potential sequences of all tails, all heads, and a mixture are indicated by the weights of the colored arrows. Lastly the probability of all generative paths leading to a given observation sum to the model evidence. The evidence of the two models is then used to select between the models given an observation.\n\n\nThis is a useful interpretation for comparing competing models of a data set to perform model selection. A model with higher Bayesian evidence is more likely to have produced the observed data, and therefore has reason to be preferred. As an application we can compare the strict fair coin model described in Eq. [eq:coin-flip-fair-likelihood] to the Bayesian model with the more permissive prior Eq. [eq:coin-flip-P-p] on the observations [eq:coin-flip-observations] of 4 out of 10 heads. The fair coin model assumes that the coin is exactly fair regardless of the observed evidence, an assumption that can be represented with a Dirac delta function prior \\[\\begin{aligned}\n    P_{\\text{fair}}(p) &= \\delta(p - 0.5).\\end{aligned}\\] In this case integrating over the prior simply evaluates at \\(p = 0.5\\) to give the model evidence \\[\\begin{aligned}\n    P_{\\text{fair}}(\\text{H},...,\\text{T}) = \\int_0^1 P(\\text{H},...,\\text{T}|p)P_{\\text{fair}}(p) dp = \\frac{1}{2^n} \\approx 0.097\\%\\end{aligned}\\] that the fair coin model would generate our sequence. In comparison the model evidence with the prior Eq. [eq:coin-flip-P-p] is \\[\\begin{aligned}\n    P_{\\text{gen}}(\\text{H},...,\\text{T}) = \\int_0^1 P(\\text{H},...,\\text{T}|p)P(p) dp = \\frac{41!(20+n_H)!(20+n_T)!}{(41 + n)!20!20!} \\approx 0.091\\%,\\end{aligned}\\] and so this more general model is overall (slightly) less likely to have generated our observations. The ratio of such evidences is known as a Bayes factor between the two models, in this case the ratio \\[\\begin{aligned}\n    \\frac{P_{\\text{gen}}(\\overbrace{\\text{H},...,\\text{T}}^{10 \\text{ flips}})}{P_{\\text{fair}}(\\text{H},...,\\text{T})} \\approx 0.93\\end{aligned}\\] less than 1 indicates that the more complex model is not justified over the fair coin. We note here that the Bayesian evidence has naturally penalized over-parametrization. Although the choice of parameter \\(p = 0.4\\) does a better job than the fair coin in isolation, it has a higher model likelihood, this peak must be weighed against all possible other values of the parameter which in this case offset that advantage.\nHad we instead observed 40 heads out of 100 flips, the Bayes factor flips to \\[\\begin{aligned}\n    \\frac{P_{\\text{gen}}(\\overbrace{\\text{H},...,\\text{T}}^{100 \\text{ flips}})}{P_{\\text{fair}}(\\text{H},...,\\text{T})} \\approx 2.25 \\label{eq:coin-flip-bayes-factor}\\end{aligned}\\] as the more flexible model is now preferred in the presence of increased evidence that the coin is not quite fair. Depending on the particular data set being considered we may prefer one model or the other.\nIn fact, given two models \\(M_1\\) and \\(M_2\\), there will always be data sets that prefer one model over the other. No model can be strictly better than another across all possible observations. To see this, consider the Bayesian evidences \\(P_1(\\vec{s})\\) and \\(P_2(\\vec{s})\\) of the two models. Both of these are normalized distributions over the possible observations that a model can generate. If we assume that the Bayesian evidence \\(P_1(\\vec{s})\\) is greater than \\(P_2(\\vec{s})\\) for any observation \\(\\vec{s}\\), both distributions can not be simultaneously normalized to 1 as \\[\\begin{aligned}\n    1 = \\sum_{\\vec{s}} P_1(\\vec{s}) &gt; \\sum_{\\vec{s}} P_2(\\vec{s}) \\not= 1.\\end{aligned}\\] Therefore both an observation \\(\\vec{s}_1\\) that favors model 1 and an observation \\(\\vec{s}_2\\) that favors model 2. In this spirit this inherent trade-off is sometimes known as the \"no free lunch\" theorem (Peel, Larremore, and Clauset 2017). When building models and performing model selection between them, we hope that \"realistic data sets\" fall within the preferred domain of our models.\nWhen considering a data set, we can also compute Bayes factors like Eq. [eq:coin-flip-bayes-factor] by taking advantage of the nested structure of our model. Since the general model is equivalent to the fair coin model at the value \\(p = 0.5\\), the Bayes factor can be written in terms of the posterior distribution at that value. Here we have \\[\\begin{aligned}\n    P(p = 0.5|\\text{H},...,\\text{T}) = \\frac{P(\\text{H},...,\\text{T}|p = 0.5)P(p = 0.5)}{P(\\text{H},...,\\text{T})} = \\frac{P_{\\text{fair}}(\\text{H},...,\\text{T})}{P_{\\text{gen}}(\\text{H},...,\\text{T})}.\\end{aligned}\\] Because of this, if our posterior distribution in the general model is meaningfully peaked at \\(p = 0.5\\), the evidence of the fair coin is greater than that of the general coin. If the posterior excludes \\(p = 0.5\\), the reverse is true.\nThis formulation is useful since it is in practice often difficult to compute the absolute Bayesian evidence of a model since it involves integrating over the high dimensional space of all possible parameter values. If we define a nested model, however, we can approximate the posterior distribution using Monte Carlo methods as demoed here relatively easily to and so compute these Bayes factors and compare models."
  },
  {
    "objectID": "notes/2025/statistical_inference.html#network-science",
    "href": "notes/2025/statistical_inference.html#network-science",
    "title": "Statistical inference review",
    "section": "Network science",
    "text": "Network science\nIn network science, this Bayesian approach is particularly powerful for model selection and parameter inference. For example, when analyzing the structure of a network, we might compare a simple random graph model (e.g., Erdős–Rényi) against a more complex description like the configuration model. The model evidence naturally weighs the goodness-of-fit against the complexity of the model and penalizes models whose extra flexibility only hampers their explanatory power.\nNested models are a particularly useful way to select between models, and are a central trick in much of my work. In each example we start with a simple model then generalize it within this Bayesian framework. By doing this we can check both whether this generalization is warranted and, if so, measure the size of the new effect, just like we infer \\(p\\) in the biased coin model. For example in work (Jerdee and Newman 2024) with Mark Newman we had applied these methods to directly compare Bradley-Terry models against minimum violation ranking by constructing a general model that includes both possibilities as special cases. By stating and incorporating our prior expectations into the inference process, a Bayesian framework enables us to handle uncertainty and validate our models in a graceful manner."
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Notes",
    "section": "",
    "text": "Text-based ramblings\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical inference review\n\n\n\n\n\n\nInference\n\n\n\nQuick primer on Bayesian inference and model selection.\n\n\n\n\n\nJul 6, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research interests",
    "section": "",
    "text": "My research leverages tools from physics, statistics, and diverse disciplines to model networks and enhance our understanding of the complex systems they represent. By modeling mechanisms, we can probe rich structures like groups and hierarchies in a wide variety of networks. My work looks to not only provide explanations for these observed patterns but also to justify and validate those inferences. By refining these methods, I ultimately aim to tighten the connection between mathematical models and qualitative, interpretable theories of system behavior.\n\nCommunity Detection\nIn real-world networks we often find communities — groups of nodes which interact with each other more frequently than with nodes outside their group. Common examples include friend groups, functional neuronal groups, or ecological niches. Despite their ubiquity, network data does not often come to us already labeled with this group structure; it must be algorithmically inferred from the network alone. I work on refining and evaluating methods to find such groups in networks, especially in situations where usual models struggle.\n\n\nHierarchies\nWhen we observe directed relationships such as dominance interactions among animals or humans, the directions of faculty hiring among universities, or wins and losses in games and sports, hierarchies routinely emerge. I use Bayesian models to not only find the order of these hierarchies, but also how unequal they are. For example, animal hierarchies are generally much stricter than those in sports or schools. By measuring effects like social inequality, I hope to better understand what drives these differences and how they affect people at the top and bottom.\n\n\nScalable Inference\nAs our models get more complicated, analysis gets harder. I’m interested in developing computational tools—like Monte Carlo methods and belief propagation—to efficiently compare models and estimate which ones best explain real network data, even for huge datasets.\n\n\nAsymptotic Understanding\nMany network problems boil down to large, random matrices—which physics has great tools for analyzing. These networks, however, can only capture dyadic interactions between pairs of nodes. “Hypergraphs” that capture higher order interactions are described by random tensors instead. I aim to better understand the behavior of these large random tensors, and its implications for network science and physics at large.\n\n\nImplementation and Outreach\nOverall, I aim to not only advance these methods, but also make them accessible. I’m especially interested in creating interactive online resources to share our findings and make network science more approachable to everyone."
  },
  {
    "objectID": "research.html#community-detection",
    "href": "research.html#community-detection",
    "title": "Research interests",
    "section": "Community detection",
    "text": "Community detection\nA central problem in network science is community detection: identifying latent groups of network nodes with similar behavior. For instance, a school friend group might be defined as a collection of students who are more frequently connected with each other than with those outside the group. This intuitive definition translates into the stochastic block model (SBM), where the model likelihood represents the probability that a given underlying friend group structure would generate the observed network. By inverting this generative model to perform inference, the SBM identifies the most probable friend groups based on the observed friendships. Although first introduced in this sociological context, the SBM is just a form of Ising model or Potts model. In this correspondence, each site represents a student. If the spin is up, the student belongs to one group; if down, they belong to the other. Just as the ground state configuration of the ferromagnetic Ising model minimizes different-spin neighbors, the best fit to the SBM minimizes unlikely friendships between different groups. Thus, many network inference questions can be recast in physical language and solved with physical tools. The same Monte Carlo algorithms used to find ground states and explore entropically typical configurations can be repurposed to fit network models. I look to refine these methods to extend the application of the stochastic model to settings where it consistently exhibits biases."
  },
  {
    "objectID": "research.html#hierarchies",
    "href": "research.html#hierarchies",
    "title": "Research interests",
    "section": "Hierarchies",
    "text": "Hierarchies\nWhen players enter a sports tournament, humans attend a high school, or chickens are put in a coop, a hierarchy tends to emerge. This pecking order might be reflected in chess game victories, unreciprocated friendships, or chicken pecks. Over the last century many models have been developed and applied to understand and predict these patterns. In recent work, we have developed a Bayesian model to infer both the order of these hierarchies and notably their degree of inequity. We have found a good deal of variation between these settings. Sports leagues tend to be fairly competitive and unpredictable, while animal hierarchies are very unequal and rigid. On this spectrum, human social hierarchies between friends or institutions tend to fall in the middle. With this ability to measure the degree of social inequality, I hope to more deeply analyze the root causes of social inequality and characterize the types of shapes the distribution of social status can take. What societal factors lead one high school to be more socially stratified than another? What are differences in outcomes among those at the top and bottom of a social hierarchy?"
  },
  {
    "objectID": "research.html#scalable-inference",
    "href": "research.html#scalable-inference",
    "title": "Research interests",
    "section": "Scalable inference",
    "text": "Scalable inference\nAs network models grow more complex, the demands on the underlying computational tools intensify. Each new model represents a competing hypothesis to explain observed data, making model selection crucial. In semi-supervised cases, where algorithm output is checked against known truths, I develop unbiased information-theoretic measures of similarity to declare winners. In unsupervised settings, ideally I aim to use Bayesian evidence—the probability that a given model generates the observed data—to adjudicate between models. This Bayesian evidence is the free energy of the corresponding physical system, and so we can apply and enhance computational physics techniques to approximate it. As data sets grow larger, Monte Carlo methods for full Bayesian inference become prohibitively expensive, necessitating approximations like mean field methods and belief propagation algorithms. Refining these methods and understanding their asymptotic performance is therefore essential to both the understanding of condensed matter and the rigorous analysis of ever larger data sets."
  },
  {
    "objectID": "research.html#asymptotic-understanding",
    "href": "research.html#asymptotic-understanding",
    "title": "Research interests",
    "section": "Asymptotic understanding",
    "text": "Asymptotic understanding\nNetwork science often deals with pairwise interactions, represented by n x n adjacency matrices where entries denote interaction strength. Random network models thus are interpretable as random matrix models, whose large n limit can be studied with free probability. For instance, the distribution of eigenvalues of random graphs with independent and identical entries famously converges to a semicircle. This control is leveraged to derive the SBM phase transition mentioned earlier. This program has been extremely successful, but many questions in network science now involve hypergraph data, which models higher-order interactions such as among triplets of nodes. These are represented by tensors instead of matrices, necessitating fundamental progress in random tensor theory."
  },
  {
    "objectID": "research.html#implementation-and-outreach",
    "href": "research.html#implementation-and-outreach",
    "title": "Research interests",
    "section": "Implementation and outreach",
    "text": "Implementation and outreach\nHolistically, my research goals encompass refining these tools and ensuring their effective application. I am particularly interested in developing online interactive materials to communicate and package these ideas. This effort not only increases visibility of our methodological advances but also enhances educational accessibility and engagement with science for broader audiences."
  }
]