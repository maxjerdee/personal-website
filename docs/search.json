[
  {
    "objectID": "papers.html",
    "href": "papers.html",
    "title": "Papers",
    "section": "",
    "text": "M. Jerdee, A. Kirkley, and M. E. J. Newman, Normalized mutual information is a biased measure for classification and community detection. Preprint arXiv:2307.01282 (2024)."
  },
  {
    "objectID": "papers.html#pre-prints",
    "href": "papers.html#pre-prints",
    "title": "Papers",
    "section": "",
    "text": "M. Jerdee, A. Kirkley, and M. E. J. Newman, Normalized mutual information is a biased measure for classification and community detection. Preprint arXiv:2307.01282 (2024)."
  },
  {
    "objectID": "papers.html#peer-reviewed-journal-publications",
    "href": "papers.html#peer-reviewed-journal-publications",
    "title": "Papers",
    "section": "Peer reviewed journal publications",
    "text": "Peer reviewed journal publications\n\nM. Jerdee, A. Kirkley, and M. E. J. Newman, Mutual information and the encoding of contingency tables. in press, Physical Review E. Preprint arXiv:2405.05393 (2024).\nM. Jerdee and M. E. J. Newman, Luck, skill, and depth of competition in games and social hierarchies. in press, Science Advances. Preprint arXiv:2312.04711 (2024). [code] [video]\nM. Jerdee, A. Kirkley, and M. E. J. Newman, Improved estimates for the number of non-negative integer matrices with given row and column sums. Proc. R. Soc. London A 480, 20230470 (2024). [code]\nA. G. Lezcano, M. Jerdee, and L. A. P. Zayas, Cardy expansion of 3d superconformal indices and corrections to the dual black hole entropy. Journal of High Energy Physics 1, 1–46 (2023).\nD. J. Binder, S. M. Chester, and M. Jerdee, ABJ correlators with weakly broken higher spin symmetry. Journal of High Energy Physics 4, 1–57 (2021).\nD. J. Binder, S. M. Chester, M. Jerdee, and S. S. Pufu, The 3d \\(\\mathcal{N} = 6\\) bootstrap: from higher spins to strings to membranes. Journal of High Energy Physics 5, 1–63 (2021).\nP. Melchior, F. Moolekamp, M. Jerdee, R. Armstrong, A. L. Sun, J. Bosch, and R. Lupton, SCARLET: Source separation in multi-band images by constrained matrix factorization. Astronomy and Computing 24, 129–142 (2018)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Max(imilian) Jerdee",
    "section": "",
    "text": "I am a Physics PhD student at the University of Michigan advised by Prof. Mark Newman and affiliated with the Center for the Study of Complex Systems. I will be starting as an Omidyar Postdoctoral Fellow at the Santa Fe Institute in Fall 2025.\nI enjoy researching topics between physics, networks, and statistics (more info here) and building somewhat educational tools and games (check them out)."
  },
  {
    "objectID": "demos/wikiguess.html",
    "href": "demos/wikiguess.html",
    "title": "WikiGuess",
    "section": "",
    "text": "An estimation game where you can guess numbers from all sorts of wikipedia excerpts. You can also play with friends online by creating a room and guessing on the same questions. In making it, there are some interesting questions about what makes a number fun to guess.\nPlay here (the multiplayer is still a bit buggy)"
  },
  {
    "objectID": "demos/parallel_tempering.html",
    "href": "demos/parallel_tempering.html",
    "title": "Parallel tempering",
    "section": "",
    "text": "Monte Carlo algorithms are often used to explore probability distributions. Particularly, the Metropolis-Hastings algorithm is an extremely widely used technique for sampling from distributions. In this story, there are a few frog characters to familiarize ourselves with.\nFirst up, explorer frog:\n\nDoesn’t give a flip\nRed\n\nNext, we have exploiter frog (also known as greedy frog):\n\nWants flies, now\nBlue\n\nFinally, our most complicated frog will be sampler frog:\n\nWants flies, most of the time (!)\nGreen\n\nIn the Metropolis-Hastings algorithm, in order to sample from a distribution proportional to \\(P(x)\\), a proposed move from a “state” \\(x\\) to some new state \\(x'\\) will be “accepted” with probability \\[P_{\\text{accept}}(x \\rightarrow x') = \\min\\left(1,\\frac{P(x)}{P(x')}\\right).\\] (really we should be sure our proposals are symmetric, but we won’t worry about that in this demonstration).\nWhat is going on with our frogs is that there is a\nSo, generally by using parallel tempering method we can quite generically improve the convergence performance of Monte Carlo methods, and therefore the efficiency of our posterior inference.\nA nice side-effect of this, however, is the ability to leverage a technique known as “calorimetry” in the Physics community for computing the free energy of a system, and as “thermodynamic integration” within the statistis community.\nSuppose that we have a general model where the\nThe fundamental identity that will come in handy is that if we define the partition function at an arbitrary \\(\\beta\\) as\n\\[ Z(\\beta) = \\int P(A|\\vec{\\theta})P(\\vec{\\theta})^\\beta d \\theta\\]\nso that notably we have that the Bayesian evidence is obtained at \\(\\beta = 1\\):\n\\[ Z(1) = \\int P(A | \\vec{\\theta}) P(\\vec{\\theta}) d \\theta = P(A), \\qquad Z(0) = \\int P(\\theta) d\\theta = 1\\]\nKey terms/concepts:\n\nSimulated annealing is generally a useful way to improve convergence."
  },
  {
    "objectID": "demos/metropolis_hastings.html",
    "href": "demos/metropolis_hastings.html",
    "title": "MCMC",
    "section": "",
    "text": "This demo is based upon Figure 12.1 from John Miller’s book A Crude Look at the Whole.\nMonte Carlo algorithms are often used to explore probability distributions. Particularly, the Metropolis-Hastings algorithm is an extremely widely used technique for sampling from distributions. In this story, there are a few frog characters to familiarize ourselves with.\nFirst up, explorer frog:\n\nRed\nDoesn’t care about flies, wanders to any lilypad.\n\nNext, we have exploiter frog (also known as greedy frog):\n\nBlue\nWants flies, now.\n\nFinally, our most complicated frog will be sampler frog:\n\nGreen\nWants flies, most of the time (!)\n\nIn the Metropolis-Hastings algorithm, in order to sample from a distribution proportional to \\(P(x)\\), a proposed move from a “state” \\(x\\) to some new state \\(x'\\) will be “accepted” with probability \\[P_{\\text{accept}}(x \\rightarrow x') = \\min\\left(1,\\frac{P(x)}{P(x')}\\right).\\] (really we should be sure our proposals are symmetric, but we won’t worry about that in this demonstration).\nIn this analogy, the states \\(x\\) are the different lilypads, and the probability \\(P(x)\\) is the number of flies at each lilypad. The frog characters then correspond to the behavior of this Monte Carlo algorithm at different values of \\(\\beta\\). Below, you can tinker with these parameters to explore how the distribution of frogs is related to the distribution of flies depending on the \\(\\beta\\) parameter."
  },
  {
    "objectID": "demos/chaos_sampler.html",
    "href": "demos/chaos_sampler.html",
    "title": "Chaos sampler",
    "section": "",
    "text": "Play Sound\n\n \nChaotic systems are everywhere in nature. These systems are characterized by the way that small changes can snowball into massive changes in behavior, rendering them hard to predict. For example, in our planet’s climate the flap of a butterfly wing in the right place and right time can cause a hurricane to form. In this interactive demo we leverage the unpredictability of a simple chaotic system known as the Lorentz attractor to remix music recordings in unpredictable ways. Particularly, we split a recording (or “sample”) of music into evenly spaced chunks. A trajectory of the Lorentz attractor is then used to associate each of these pieces of the recording to a point in the system’s phase space. The music is then shuffled by continuing to run the trajectory and playing the chunk associated with the nearest written point in phase space at each step. By harnessing this chaos as a creative force we can play with this ubiquitous feature of the world in an accessible way.\nThis demo made during the 2024 SFI Complex Systems Summer School as part of the Art and Complexity Collective, and was inspired by Liz Bradley’s talks on Nonlinear Dynamics. Particularly, heavy inspiration taken from Diana Dabby’s work (paper, video) on chaotic variations using the Lorentz attractor."
  },
  {
    "objectID": "demos/game_of_life.html",
    "href": "demos/game_of_life.html",
    "title": "Game of life",
    "section": "",
    "text": "Conway’s game of life is a compact, elegant example of how simple principles can generate complex behaviors. The “game” is played on a 2-dimensional grid, and loosely simulates changes in the population of a species over space and time. Each grid square may either be “alive” or “dead” at each time interval. Only if a grid cell has between 3 and 5 of its 8 neighbors alive will it be alive and happy on the next turn, otherwise it will die of either under- or over-crowding. From this simple rule alone, very complicated behavior can be observed. In this demo we simulate this game of life and play sounds when each of the grid cells is alive. We also mirror our simulation on a Novation Launchpad, an 8 x 8 grid of buttons that can light up with colors. We hope that by creating a unified physical, auditory, and visual experience we have an engaging mode to play with this toy example of how complexity can emerge.\nThis demo made during the 2024 SFI Complex Systems Summer School as part of the Art and Complexity Collective.\nIf you plug in a Novation Mini [MK3], you can interact with this demonstration live. That is also what the request to “reprogram your MIDI devices” is about, otherwise you can select “Block” and simply click on the buttons within the browser.\n\n\n\nExample of asking for permissions, unless you are plugging in a Launchpad can select to Block"
  },
  {
    "objectID": "demos/metropolis_hastings_flat.html",
    "href": "demos/metropolis_hastings_flat.html",
    "title": "MCMC Flat",
    "section": "",
    "text": "Monte Carlo algorithms are often used to explore probability distributions. Particularly, the Metropolis-Hastings algorithm is an extremely widely used technique for sampling from distributions. In this story, there are a few frog characters to familiarize ourselves with.\nFirst up, explorer frog:\n\nDoesn’t give a flip\nRed\nHot\n\nNext, we have exploiter frog (also known as greedy frog):\n\nWants flies, now\nBlue\nCold\n\nFinally, our most complicated frog will be sampler frog:\n\nWants flies, most of the time (!)\nGreen\nJust right\n\nIn the Metropolis-Hastings algorithm, in order to sample from a distribution proportional to \\(P(x)\\), a proposed move from a “state” \\(x\\) to some new state \\(x'\\) will be “accepted” with probability \\[P_{\\text{accept}}(x \\rightarrow x') = \\min\\left(1,\\frac{P(x)}{P(x')}\\right).\\] (really we should be sure our proposals are symmetric, but we won’t worry about that in this demonstration).\nWhat is going on with our frogs is that there is a\nSo, generally by using parallel tempering method we can quite generically improve the convergence performance of Monte Carlo methods, and therefore the efficiency of our posterior inference.\nA nice side-effect of this, however, is the ability to leverage a technique known as “calorimetry” in the Physics community for computing the free energy of a system, and as “thermodynamic integration” within the statistis community.\nSuppose that we have a general model where the\nThe fundamental identity that will come in handy is that if we define the partition function at an arbitrary \\(\\beta\\) as\n\\[ Z(\\beta) = \\int P(A|\\vec{\\theta})^\\beta P(\\vec{\\theta}) d \\theta\\]\nso that notably we have that the Bayesian evidence is obtained at \\(\\beta = 1\\):\n\\[\n\\begin{align*}\nZ(1) &= \\int P(A | \\vec{\\theta}) P(\\vec{\\theta}) d \\theta = P(A), \\\\\nZ(0) &= \\int P(\\theta) d\\theta = 1\n\\end{align*}\n\\]\nTODO:\n\nSwitch the frogs to a raster render to improve efficiency Implement as an svg with a background color that can be changed along with a png on top that handles the hard outline and eyes, and the blush and shading as opacity layers.\nMake an annealing option that cools down all of the frogs at once (implement as an annealing speed slider)\nReset accumulation histogram when changing beta\nSnap the inverse temperature slier around beta = 1 (unless annealing)\nCreate an asset of spins arranging in order to explain the temperature and tempering idea\nAdd left/right buttons in the top left in order to flip through examples (disable these in the notes post) [Add the ability to highlight these buttons]\nExamples (include a list in the demo page, designed to be written in a more technical way to describe what is going on likely for those who have already seen this stuff before. Link to the notes post for a more pedagogical explanation.)\n\nSingle red frog [triangle]\nMany red frogs, high simulation speed\nSingle blue frog\nMany blue frogs, high simulation speed\nSingle green frog\nMany green frogs, high simulation speed\nMany red frogs, high speed [line]\nBlue frogs, high speed\nGreen frogs, high speed (introduce the idea of mixing time)\nSimulated annealing, medium speed (start on red, cool down)\nSimulated annealing, fast speed"
  },
  {
    "objectID": "demos/relativity.html",
    "href": "demos/relativity.html",
    "title": "Special relativity",
    "section": "",
    "text": "When you move faster, your personal (relative!) time slows down. This is one of the many suprising consequences of the theory of relativity, that space and time are intwined into a single spacetime and that the actions we take in one dimension (moving through space) can impact our trajectory through the other (moving through time). This demo uses your phone’s accelerometer data in order to estimate your velocity, and so estimate how much your time slows down as you walk around.\nThis demo was built as part of the Fall 2021 Science Communication Fellows program at the University of Michigan Museum of Natural History."
  },
  {
    "objectID": "demos.html",
    "href": "demos.html",
    "title": "Demos",
    "section": "",
    "text": "Assorted projects I have worked on, generally try to explore a neat idea in an interactive way.\n\n\n\n\n\n\n\n\n\n\n\n\nChaos sampler\n\n\nUse a chaotic system to chop up beat samples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGame of life\n\n\nMake music with Conway’s game of life\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMCMC\n\n\nExplore Monte Carlo methods with hopping frogs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMCMC Flat\n\n\nExplore Monte Carlo methods with hopping frogs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParallel tempering\n\n\nExplore Monte Carlo methods with hopping frogs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpecial relativity\n\n\nUse your phone to observe how much your time slows down as you move around\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWikiGuess\n\n\nCompete with friends to estimate numbers from Wikipedia\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Notes",
    "section": "",
    "text": "Text-based ramblings\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNetwork structures\n\n\n\n\n\n\nReview\n\n\n\nAn introduction to networks and the group and hierarchy structures they reveal\n\n\n\n\n\nJul 16, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nInformation theory\n\n\n\n\n\n\nInformation theory\n\n\nReview\n\n\n\nBasics of information theory: Shannon and Kolmogorov\n\n\n\n\n\nJul 15, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical physics\n\n\n\n\n\n\nPhysics\n\n\nInference\n\n\nReview\n\n\n\nFoundations of statistical physics from energy to entropy, and their links to model inference\n\n\n\n\n\nJul 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical inference\n\n\n\n\n\n\nInference\n\n\nReview\n\n\n\nQuick primer on Bayesian inference, model selection, and validation\n\n\n\n\n\nJul 11, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research interests",
    "section": "",
    "text": "My research leverages tools from physics, statistics, and diverse disciplines to model networks and enhance our understanding of the complex systems they represent. By modeling mechanisms, we can probe rich structures like groups and hierarchies in a wide variety of networks. My work looks to not only provide explanations for these observed patterns but also to justify and validate those inferences. By refining these methods, I ultimately aim to tighten the connection between mathematical models and qualitative, interpretable theories of system behavior.\n\nCommunity detection\nIn real-world networks we often find communities — groups of nodes which interact with each other more frequently than with nodes outside their group. Common examples include friend groups, functional neuronal groups, or ecological niches. Despite their ubiquity, network data does not often come to us already labeled with this group structure; it must be algorithmically inferred from the network alone. I work on refining and evaluating methods to find such groups in networks, especially in situations where usual models struggle.\n\n\nHierarchies\nWhen we observe directed relationships such as dominance interactions among animals or humans, the directions of faculty hiring among universities, or wins and losses in games and sports, hierarchies routinely emerge. I use Bayesian models to not only find the order of these hierarchies, but also how unequal they are. For example, animal hierarchies are generally much stricter than those in sports or schools. By measuring effects like social inequality, I hope to better understand what drives these differences and how they affect people at the top and bottom.\n\n\nScalable inference\nAs our models get more complicated, analysis gets harder. I’m interested in developing computational tools—like Monte Carlo methods and belief propagation—to efficiently compare models and estimate which ones best explain real network data, even for huge datasets.\n\n\nAsymptotic understanding\nMany network problems boil down to large, random matrices—which physics has great tools for analyzing. These networks, however, can only capture dyadic interactions between pairs of nodes. “Hypergraphs” that capture higher order interactions are described by random tensors instead. I aim to better understand the behavior of these large random tensors, and its implications for network science and physics at large.\n\n\nImplementation and outreach\nOverall, I aim to not only advance these methods, but also make them accessible. I’m especially interested in creating interactive online resources to share our findings and make network science more approachable to everyone."
  },
  {
    "objectID": "notes/2025/statistical_inference.html",
    "href": "notes/2025/statistical_inference.html",
    "title": "Statistical inference",
    "section": "",
    "text": "In this note we review the basics of statistical inference using simple toy models for flipping a coin. The ideas we review, like Bayesian inference and model selection, are widely used in network science to understand more complex systems. The presentation here is adapted from my PhD thesis, alongside reviews of statistical physics and information theory."
  },
  {
    "objectID": "notes/2025/statistical_inference.html#a-fair-coin",
    "href": "notes/2025/statistical_inference.html#a-fair-coin",
    "title": "Statistical inference",
    "section": "A fair coin",
    "text": "A fair coin\nSuppose we perform a simple experiment and record whether a two-sided coin lands \"heads\" or \"tails\" over \\(n\\) flips. Before the experiment begins we might hypothesize that we hold a fair coin: that all coin flips are independent and result in heads with probability \\(p = 0.5\\) and tails with probability \\(1 - p = 0.5\\). This natural assumption defines a model by assigning a likelihood of observing any particular sequence of heads and tails over the \\(n\\) trials\n\n\\[\n\\begin{aligned}\n    P(\\overbrace{\\text{H},...,\\text{T}}^{n \\text{ flips}}|p = 0.5) = \\frac{1}{2^n}. \\end{aligned}\n\\tag{1}\\]\n\nAlthough all possible sequences of \\(n\\) outcomes are equally likely to appear under this model, certain observations may lead us to doubt our initial hypothesis. Suppose every coin flip we observe lands on heads. Although a single heads does not raise suspicion, 10 heads in a row start to be quite surprising at a probability of \\(\\sim 0.1\\%\\). And after 100 fair heads in a row we may start to question our place in the universe. The question arises: how many heads should we observe before abandoning our initial conjecture? And if the coin is not fair, what is its nature? Statistics provides many frameworks for approaching these questions.\nIn the frequentist approach our initial assumption of fairness is called a null model (or \"null hypothesis\") which our experiment may then \"reject.\" In this picture we define a test statistic that captures some surprising aspect of our observed data, in this case the unusually large number of heads \\(n_H = n\\) observed. We then compute the likelihood that the null model could produce a result with a test statistic at least as extreme as that observed, known as the p-value. If this p-value is small it is unlikely the null model alone could generate an observation similar to ours. This improbability suggests our model is lacking and serves as grounds to \"reject\" it in favor of some alternative model more likely to have produced our observation.\nIn linear regression, these p-values are often reported to reject the possibility of a slope of 0, of no relation between two variables. In network science, simple null models are likewise used to demonstrate that some observed structural feature like community or hierarchy requires a richer model to reproduce.\nFor a fair coin the probability of observing \\(n_H\\) heads out of \\(n\\) trials is given by the binomial distribution \\[\\begin{aligned}\n    P(n_H|p = 0.5) = \\frac{1}{2^n} \\binom{n}{n_H}.\\end{aligned}\\] The p-value, the chance of observing some number of heads \\(n_H'\\) greater than or equal to \\(n_H\\), is then\n\n\\[\\begin{aligned}\n    P(n_H' \\geq n_H|p = 0.5) = \\frac{1}{2^n} \\sum_{n_H' \\geq n_H} \\binom{n}{n_H'}.\\end{aligned}\n\\]\n\nAs shown in Figure 1, over 10 fair coin flips we expect an average of 5 heads in this model although small fluctuations such as 4 or 6 heads are also common. More lopsided outcomes are increasingly unlikely, so that the p-values of observing larger \\(n_H\\) approach zero.\n\n\n\n\n\n\nFigure 1: Distribution of the number of heads \\(n_H\\) observed over  \\(n = 10\\) flips of a fair coin and the resulting p-value probabilities of observing at least \\(n_H\\) heads. The region where the p-value is less than 0.05 is plotted in gray, a threshold to reject the null hypothesis of a fair coin.\n\n\n\nA smaller p-value yields a more statistically significant rejection of the null hypothesis. A threshold of \\(P &lt; 0.05\\) is often used as a minimum standard, in which case we would reject the hypothesis of a fair coin in our experiment if 9 or 10 of the coin flips land heads. This demarcation at \\(0.05\\) is arbitrary; however small there is always some chance that a genuine fair coin produced an observation as unusual as the one made. We would expect to obtain a p-value less than 0.05 and so reject the model in 1 of 20 experiments conducted with even a truly fair coin. The choice of p-value threshold reflects a tolerance for this possibility that our rejection is the result of chance rather than a genuine signal."
  },
  {
    "objectID": "notes/2025/statistical_inference.html#a-biased-coin",
    "href": "notes/2025/statistical_inference.html#a-biased-coin",
    "title": "Statistical inference",
    "section": "A biased coin",
    "text": "A biased coin\nIf such significance testing has led us to doubt that the coin is fair, we may consider an alternative hypothesis. We could instead model a biased coin whose flips are still independent but land heads with some fixed probability \\(p \\in [0,1]\\) and tails with probability \\(1 - p\\). Under this assumption the likelihood of a sequence with \\(n_\\text{H}\\) heads and \\(n_\\text{T} = n - n_\\text{H}\\) tails is\n\n\\[\\begin{aligned}\nP(\\overbrace{\\text{H},...,\\text{T}}^{n_\\text{H} \\text{ heads}}|p) = p^{n_\\text{H}}(1-p)^{n - n_\\text{H}}. \\end{aligned} \\tag{2}\\]\n\nThis defines a nested model by generalizing the fair coin as the special case \\(p = 0.5\\). Thus the original fair coin model can be directly compared against other choices of the parameter \\(p\\) which each represent possible coins of varying levels of bias towards landing heads or tails. Before the experiment begins we imagine all these coins are possible and aim to infer the \"true\" value of \\(p\\) realized by our coin based on the observations.\nTo apply this model suppose that after 10 trials we observe a sequence\n\n\\[\\begin{aligned}\n    (\\text{H}, \\text{H}, \\text{T}, \\text{T}, \\text{H}, \\text{T}, \\text{T}, \\text{H}, \\text{T}, \\text{T}), \\end{aligned} \\tag{3}\\]\n\nnow a mixture of \\(n_\\text{H} = 4\\) heads and \\(n_{\\text{T}} = 6\\) tails. We fit the model likelihood Eq. 2 to this data by finding the value of \\(p\\) which would maximize the probability that our observed sequence occurred. For this model this is simply equal to the observed fraction of heads\n\n\\[\\begin{aligned}\n    \\hat{p}_{\\text{ML}} = \\frac{n_\\text{H}}{n}\\end{aligned}\\]\n\nwhere the \"ML\" subscript indicates that this is the maximum likelihood estimate of \\(p\\). Our sequence of observations Eq. 3 gives the estimate \\(\\hat{p}_{\\text{ML}} = 0.4\\).\nAlthough this \\(\\hat{p}_{\\text{ML}}\\) is the single parameter value most likely to have generated our observations, it is unclear how seriously we should take the estimate. If the coin was in fact fair, \\(p = 0.5\\), the slight imbalance towards tails we observe could very well be a random fluctuation in our small sample size as seen in Figure 1. In our earlier language the p-value is not small enough to warrant rejecting the null hypothesis of a fair coin. To conclude that the one \"true\" value of \\(p\\) is 0.4, for example to predict that 400 of the next 1000 flips will be heads, would likely overfit the data. As a more extreme example if we only observe a single coin flip which happens to land heads, the maximum likelihood estimate would conclude that \\(\\hat{p}_{\\text{ML}} = 1\\) and thus that the coin will always land heads. There should be uncertainty in our conclusions, particularly when they are based on such little evidence."
  },
  {
    "objectID": "notes/2025/statistical_inference.html#bayesian-inference",
    "href": "notes/2025/statistical_inference.html#bayesian-inference",
    "title": "Statistical inference",
    "section": "Bayesian inference",
    "text": "Bayesian inference\nBy adopting a Bayesian perspective we can naturally represent this uncertainty in our inferences. In this framework we critically never exclude the possibility of any potential parameter value. Rather we infer from the data that some parameter values are more probable than others. At each stage of the experiment we represent our belief of likely values as a distribution over all possibilities rather than a single best-fit point.\nBefore considering the data we define a prior distribution (or \"prior\") over the parameters that reflects our initial assumptions. Depending on the context of our experiment we may have quite different expectations that will rightfully influence the conclusions we draw. For example if we use a standard legal tender coin we may hesitate to conclude that the coin is notably biased, even after observing 10 heads in a row. However, if the flips instead involve either a hypothetical or peculiar-looking coin there may be more room for doubt. The form of the prior distribution precisely specifies our initial assumptions.\nFor our example we will adopt a prior assumption that the coin is likely to be fair or approximately fair. Although a physical coin could conceivably be as biased as \\(p = 0.4\\), we expect a balanced \\(p = 0.5\\) to be far more likely. To represent this we define a prior distribution over possible \\(p\\) peaked at \\(p = 0.5\\), particularly a beta distribution \\(p \\sim B(20,20)\\)\n\n\\[\\begin{aligned}\n    P(p) = \\frac{41!}{20!20!} p^{20}(1-p)^{20}, \\end{aligned} \\tag{4}\\]\n\nplotted in Figure 2c. This particular form of the prior and the number 20 are arbitrarily chosen for this example, although they represent a reasonable assumption for this scenario.\n\n\n\n\n\n\nFigure 2: (a) Posterior distributions of the parameter \\(p\\) after observing 0 coin flips (a.k.a. prior distribution), 10, and 100 coin flips. In each experiment 40% of trials are heads, a ratio marked by the vertical line in each plot. The posterior distributions are proportional to the product of the likelihood (b) and prior (c) over \\(p\\). As more observations are made the posterior is increasingly informed by the model likelihood.\n\n\n\nAs we observe coin flips we update these prior beliefs to reflect new data and arrive at a new posterior distribution of likely parameter values. Using Bayes’ law this posterior distribution is proportional to the product of the model likelihood and the initial prior distribution as\n\n\\[\\begin{aligned}\n    P(p|\\text{H},...,\\text{T}) \\propto P(\\text{H},...,\\text{T}|p)P(p). \\end{aligned} \\tag{5}\\]\n\nThis form ensures that when no data is observed the posterior distribution is equal to the prior assumption \\(P(p)\\). As flips are recorded the influence of the prior distribution wanes as the posterior is dominated by the model likelihood \\(P(\\text{H},...,\\text{T}|p)\\). Figure 2 demonstrates this shift in the posterior distribution (a) from the prior (c) to the likelihood (b) after making 0, 10, and 100 observations at a ratio of 40% heads. As more observations are made with an empirical probability of \\(p = 0.4\\), our posterior distribution becomes increasingly concentrated around that belief. We must however observe sufficient evidence to overcome the strength of our initial assumptions before coming to that conclusion with certainty.\nThe width and form of the posterior distribution gives a full picture of the uncertainty of our the inference. Although the posterior distribution in Figure 2a is tightly concentrated around a probability \\(p \\sim 0.4\\) after observing 100 flips, we retain some ambiguity if the parameter might deviate slightly from that peak. Nonetheless it is often useful to report the single choice of parameter most favored by the posterior distribution, known as the maximum a posteriori (MAP) estimate. For our example and choice of prior Eq. 4 this is\n\n\\[\\begin{aligned}\n    \\hat{p}_{\\text{MAP}} = \\frac{n_H + 20}{n + 40}. \\end{aligned} \\tag{6}\\]\n\nThe earlier interplay between the prior and likelihood is present in this estimate. When no coin flips are observed we obtain the prior assumption \\(\\hat{p}_{\\text{MAP}} = 0.5\\). As the number of observations \\(n\\) increases, this Bayesian estimate approaches the frequentist maximum likelihood estimate, \\(\\hat{p}_{\\text{MAP}} \\rightarrow \\frac{n_H}{n} = \\hat{p}_{\\text{ML}}\\)."
  },
  {
    "objectID": "notes/2025/statistical_inference.html#bayesian-evidence",
    "href": "notes/2025/statistical_inference.html#bayesian-evidence",
    "title": "Statistical inference",
    "section": "Bayesian evidence",
    "text": "Bayesian evidence\nTo complete Bayes’ law and fully specify the posterior distribution we normalize Eq. 5 as\n\n\\[\\begin{aligned}\n    P(p|\\text{H},...,\\text{T}) &= \\frac{P(\\text{H},...,\\text{T}|p)P(p)}{P(\\text{H},...,\\text{T})} \\end{aligned} \\tag{7}\\]\n\nwhere\n\n\\[\\begin{aligned}\n    P(\\text{H},...,\\text{T}) = \\int_0^1 P(\\text{H},...,\\text{T}|p)P(p) dp\\end{aligned}\\]\n\nis the evidence (or \"marginal likelihood\") of the model. This evidence can be interpreted as the probability of arriving at the observations \\(\\text{H},...,\\text{T}\\) through the two stage process of first choosing a parameter \\(p\\) from the prior \\(P(p)\\) then generating the data from the model likelihood \\(P(\\text{H},...,\\text{T}|p)\\). By integrating over all possible parameters \\(p\\), the Bayesian evidence is equal to the total probability that a model generates the observed data from our prior assumptions. A cartoon of this generative process is given in Figure 3.\n\n\n\n\n\n\nFigure 3: Schematic of the full generative process of the fair and biased coin models. A parameter value \\(p\\) is first drawn from the appropriate prior \\(P(p)\\), represented by the thicknesses of the top gray arrows to example parameter values \\(p = 0.4, 0.5,\\) and \\(0.6\\). This choice of parameter then generates the observed data according to the model likelihood \\(P(\\text{H},...,\\text{T}|p)\\), represented by the weights of the colored arrows to the example observations of all tails, a mixture, and all heads. Lastly we compute the model evidence of each observation by summing over all generative paths leading to that outcome. For some data the fair model performs better while for others the biased coin is preferred.\n\n\n\nThis is a useful interpretation for comparing competing models of a data set to perform model selection. A model with higher Bayesian evidence is more likely to have produced the observed data, and therefore has reason to be preferred. As an application we can compare the strict fair coin model described in Eq. 1 to the Bayesian model with the more permissive prior Eq. 4 on the observations Eq. 3 of 4 out of 10 heads. The fair coin model assumes that the coin is exactly fair regardless of the observed evidence, an assumption that can be represented with a Dirac delta function prior\n\n\\[\\begin{aligned}\n    P_{\\text{fair}}(p) &= \\delta(p - 0.5).\\end{aligned}\\]\n\nIn this case integrating over the prior simply evaluates at \\(p = 0.5\\) to give the model evidence\n\n\\[\\begin{aligned}\n    P_{\\text{fair}}(\\text{H},...,\\text{T}) &= \\int_0^1 P(\\text{H},...,\\text{T}|p)P_{\\text{fair}}(p) dp \\\\\n    &= \\frac{1}{2^n} \\approx 0.097\\%\\end{aligned}\\]\n\nthat the fair coin model would generate our sequence. In comparison the model evidence with the prior Eq. 4 is\n\n\\[\\begin{aligned}\n    P_{\\text{gen}}(\\text{H},...,\\text{T}) &= \\int_0^1 P(\\text{H},...,\\text{T}|p)P(p) dp \\\\\n    &= \\frac{41!(20+n_H)!(20+n_T)!}{(41 + n)!20!20!} \\\\\n    &\\approx 0.091\\%,\\end{aligned}\\]\n\nand so this more general model is overall (slightly) less likely to have generated our observations. The ratio of such evidences is known as the Bayes factor between two models. Here the ratio\n\n\\[\\begin{aligned}\n    \\frac{P_{\\text{gen}}(\\overbrace{\\text{H},...,\\text{T}}^{10 \\text{ flips}})}{P_{\\text{fair}}(\\text{H},...,\\text{T})} \\approx 0.93\\end{aligned}\\]\n\nless than 1 indicates that the more complex model is not justified over the fair coin. Note that the Bayesian evidence has naturally penalized over-parametrization. Although the choice of parameter \\(p = 0.4\\) does a better job than the fair coin in isolation, it has a higher model likelihood, this peak must be weighed against all possible other values of the parameter which in this case offset that advantage.\nHad we instead observed 40 heads out of 100 flips, the Bayes factor flips to\n\n\\[\\begin{aligned}\n    \\frac{P_{\\text{gen}}(\\overbrace{\\text{H},...,\\text{T}}^{100 \\text{ flips}})}{P_{\\text{fair}}(\\text{H},...,\\text{T})} \\approx 2.25 \\end{aligned} \\tag{8}\\]\n\nas the more flexible model is now preferred in the presence of increased evidence that the coin is not quite fair. Depending on the particular data set being considered we may prefer one model or the other.\nIn fact, given two models \\(M_1\\) and \\(M_2\\), there will always be data sets that prefer one model over the other. No model can be strictly better than another across all possible observations. To see this, consider the Bayesian evidences \\(P_1(\\boldsymbol{s})\\) and \\(P_2(\\boldsymbol{s})\\) of the two models. Both of these are normalized distributions over the possible observations that a model can generate. If we suppose that the Bayesian evidence \\(P_1(\\boldsymbol{s})\\) is greater than \\(P_2(\\boldsymbol{s})\\) for all observations \\(\\boldsymbol{s}\\), both distributions can not be simultaneously normalized to 1 as\n\n\\[\\begin{aligned}\n    1 = \\sum_{\\boldsymbol{s}} P_1(\\boldsymbol{s}) &gt; \\sum_{\\boldsymbol{s}} P_2(\\boldsymbol{s}) \\not= 1.\\end{aligned}\\]\n\nThere therefore exists both an observation \\(\\boldsymbol{s}_1\\) that favors model 1 and an observation \\(\\boldsymbol{s}_2\\) that favors model 2. In this spirit, this inherent trade-off is sometimes known as the \"no free lunch\" theorem (Peel, Larremore, and Clauset 2017). When building models and performing model selection between them, we hope that \"realistic data sets\" fall within the preferred domain of our models.\nWhen considering a data set, we can also compute Bayes factors like Eq. 8 by taking advantage of the nested structure of our model. Since the general model is equivalent to the fair coin model at the value \\(p = 0.5\\), the Bayes factor can be written in terms of the posterior distribution at that value. Explicitly we have\n\n\\[\\begin{aligned}\n    P(p = 0.5|\\text{H},...,\\text{T}) &= \\frac{P(\\text{H},...,\\text{T}|p = 0.5)P(p = 0.5)}{P(\\text{H},...,\\text{T})} \\\\\n    &= \\frac{P_{\\text{fair}}(\\text{H},...,\\text{T})}{P_{\\text{gen}}(\\text{H},...,\\text{T})}.\\end{aligned}\n\\]\n\nBecause of this, if our posterior distribution in the general model is meaningfully peaked at \\(p = 0.5\\), the evidence of the fair coin is greater than that of the general coin. If the posterior excludes \\(p = 0.5\\), the reverse is true.\nThis formulation is useful since it is often difficult in practice to compute the absolute Bayesian evidence of a model as it involves integrating over the high dimensional space of all possible parameter values. If we define a nested model, however, we can approximate the posterior distribution using Monte Carlo methods as demoed here relatively easily and so compute these Bayes factors to compare models."
  },
  {
    "objectID": "notes/2025/statistical_inference.html#network-science",
    "href": "notes/2025/statistical_inference.html#network-science",
    "title": "Statistical inference",
    "section": "Network science",
    "text": "Network science\nIn network science, this Bayesian approach is particularly powerful for model selection and parameter inference. For example, when analyzing the structure of a network, we might compare a simple random graph model (e.g., Erdős–Rényi) against a more complex description like the configuration model. The model evidence naturally weighs the goodness-of-fit against the complexity of the model and penalizes models whose extra flexibility only hampers their explanatory power.\nNested models are a particularly useful way to select between models, and are a central trick in much of my research. In each application we start with a simple model then generalize it within this Bayesian framework. By doing this we can check both whether this generalization is warranted and, if so, measure the size of the new effect, just like we infer \\(p\\) in the biased coin model. For example in work (Jerdee and Newman 2024) with Mark Newman we had applied these methods to directly compare Bradley-Terry models against minimum violation ranking by constructing a general model that includes both possibilities as special cases. By stating and incorporating our prior expectations into the inference process, a Bayesian framework enables us to handle uncertainty and validate our models in a graceful manner."
  },
  {
    "objectID": "notes/2025/statistical_physics.html",
    "href": "notes/2025/statistical_physics.html",
    "title": "Statistical physics",
    "section": "",
    "text": "In this note we review the fundamentals of equilibrium statistical mechanics and their application to statistical modeling. We demonstrate how concepts of probability and inference can be usefully cast in the language of physics and energy. By leveraging this correspondence, we can import analytic and computational tools from physics to tackle statistical questions.\nWe ground our exploration in the simple statistical setting of independent flips of a coin with probability \\(p\\) of landing heads. As discussed in this note, the model likelihood of observing a sequence \\(\\boldsymbol{s}\\) with \\(n_H\\) heads out of \\(n\\) flips is\nWe define an physical model equivalent to this statistical one and explore how this physical perspective motivates alternative types of models and inference techniques. The presentation here is adapted from my PhD thesis, alongside reviews of statistical inference and information theory"
  },
  {
    "objectID": "notes/2025/statistical_physics.html#energy-and-its-conservation",
    "href": "notes/2025/statistical_physics.html#energy-and-its-conservation",
    "title": "Statistical physics",
    "section": "Energy and its conservation",
    "text": "Energy and its conservation\nPhysics often describes systems in terms of a configuration space of possible states. In the coin flip example, the state of the system (or \"configuration\") is the sequence of \\(n\\) observed outcomes. We represent this configuration as a vector \\(\\boldsymbol{s}\\), where each entry \\(s_i = 1\\) indicates that flip \\(i = 1,...,n\\) landed heads, and \\(s_i = 0\\) represents tails. The configuration space of coin flips is thus comprised of all possible sequences of \\(n\\) flips, forming all binary vectors of length \\(n\\).\nThis notation elicits the Ising model fundamental to statistical mechanics. The configurations of this model consist of atomic \"spins\" that reside at one of \\(n\\) \"sites,\" each in either its excited state \\(s_i = 1\\) or ground state \\(s_i = 0\\)1. We assign the excited state an energy of 1 and the ground state energy 0, as shown in Figure 1. The total energy of the system, referred to as the Hamiltonian, is then the number of excited states (or heads)\n\n\\[\\begin{aligned}\n    H(\\boldsymbol{s}) = \\sum_{i=1}^n \\left(0 \\delta_{s_i 0} + 1 \\delta_{s_i 1}\\right) = n_H. \\end{aligned} \\tag{2}\\]\n\nA more general Ising model would include coupling energies between neighboring spins, but we omit them here to maintain independent coin flips.\n\n\n\n\n\n\n\nFigure 1: Physical interpretation of coin flips. Heads (\"H\") have energy \\(E = 1\\) while tails (\"T\") have energy \\(E = 0\\), so that the overall energy of the configuration \\(\\boldsymbol{s}\\) is equal to the number of observed heads, \\(H(\\boldsymbol{s}) = n_H\\).\n\n\n\nIn an isolated system the energy Eq. 2 is always conserved, a fundamental law of physics. Although changes in individual spin states can occur, they must be counterbalanced by changes in other spins to maintain the \"global\" energy of the system. Let \\(E\\) be this constant energy of the system, so that only configurations \\(\\boldsymbol{s}\\) where \\(H(\\boldsymbol{s}) = E\\) are permitted, cases with \\(n_H = E\\) excited states. The number of possible system configurations (also known as microstates) that satisfy this condition is then the number of ways to arrange the \\(n_H = E\\) excited states among the \\(n\\) sites,\n\n\\[\\begin{aligned}\n    \\Omega(E) = \\binom{n}{E}. \\end{aligned} \\tag{3}\\]\n\nStatistical mechanics fundamentally postulates that when a system is in equilibrium, all these microstates of the same total energy are equally likely to appear. This assumption defines equilibrium statistical mechanics, the framework we adopt in this note. Under this postulate the probability of observing any specific configuration \\(\\boldsymbol{s}\\) is therefore\n\n\\[\\begin{aligned}\n    P(\\boldsymbol{s}|E) = \\binom{n}{E}^{-1}. \\end{aligned} \\tag{4}\\]\n\nThis uniform distribution over all configurations of a fixed energy is known as the microcanonical ensemble. As a distribution over possible sequences of coin flips, it can also be interpreted as a microcanonical model where the continuous local probability of heads \\(p\\) has been replaced by the discrete global number of heads \\(n_H = E\\).\nMany statistical models are framed in this microcanonical form, where the \"parameters\" are globally observed quantities. While this formulation may not be well-suited for the coin flip example, where we have little reason to expect a \"conservation of heads,\" it arises in other statistical settings where certain properties are conserved across random realizations. For instance, in a network of sports matches the total number of games played each season remains constant each year, even though the pattern of connections among teams changes."
  },
  {
    "objectID": "notes/2025/statistical_physics.html#entropy",
    "href": "notes/2025/statistical_physics.html#entropy",
    "title": "Statistical physics",
    "section": "Entropy",
    "text": "Entropy\nTo return to the independent coin flip model we must broaden the physical picture. Realistically most physical systems are not truly isolated but rather exchange energy with their environment in a setting known as the canonical ensemble. In physical language the original system is now a subsystem in equilibrium with a large thermal bath. Although the combined system of both the subsystem and the thermal bath must still conserve overall energy, our subsystem of interest can gain and lose energy to the thermal bath.\nFigure 2 illustrates this set up for the coin flip example. Let \\(\\tilde{\\boldsymbol{s}}\\) represent the configuration of the thermal bath, containing \\(\\tilde{n}\\) spins and energy \\(\\tilde{E} = H(\\tilde{\\boldsymbol{s}})\\) which counts the number of excited states in the bath. Including our subsystem \\(\\boldsymbol{s}\\), the full system \\((\\boldsymbol{s},\\tilde{\\boldsymbol{s}})\\) then has \\(n + \\tilde{n}\\) total spins and total energy \\(E_T = H(\\boldsymbol{s},\\tilde{\\boldsymbol{s}}) = E + \\tilde{E}\\). Since this total energy is conserved, the full system \\((\\boldsymbol{s},\\boldsymbol{s}')\\) is uniformly distributed according to the microcanonical ensemble Eq. 4 of \\(E_T\\) excited states on \\(n + \\tilde{n}\\) sites,\n\n\\[\\begin{aligned}\n    P(\\boldsymbol{s},\\tilde{\\boldsymbol{s}}|E_T) &= \\binom{n + \\tilde{n}}{E_T}^{-1}. \\end{aligned} \\tag{5}\\]\n\nAlthough each pair \\((\\boldsymbol{s},\\boldsymbol{s}')\\) with total energy \\(E_T\\) is equally likely, certain subsystem energies \\(E\\) are more likely than others. Figure 2 demonstrates this effect with two example configurations. In panel (a) the excited states (\"H\") are all contained within the thermal bath \\(\\tilde{\\boldsymbol{s}}\\), meaning that the subsystem \\(\\boldsymbol{s}\\) has its lowest possible energy \\(E = 0\\) and the thermal bath has the full energy \\(E_T\\). In panel (b) the excited states are evenly distributed between the subsystem and the bath, and the subsystem has absorbed some energy from the bath. Across all possible configurations of the joint subsystem-bath system, imbalanced distributions like (a) are less common than balanced ones (b). Random configurations of the overall ensemble are therefore likely to be balanced. As a result, if our subsystem begins in its ground state (a) it will tend to thermalize into a configuration like (b) with evenly distributed excited states.\n\n\n\n\n\n\nFigure 2: Thermalization of a subsystem (of coin flips) in contact with a thermal bath. In panel (a) all excited states (\"H\") are contained in the bath, an unusual configuration with overall entropy \\(S(E) + \\tilde{S}(\\tilde{E}) = 8.77\\). Panel (b) has a more typical, even arrangement of the excited states, reflected in the higher total entropy \\(S(E) + \\tilde{S}(\\tilde{E}) = 10.82\\). If the subsystem begins in the ground state (a) it will thus likely thermalize to the equilibrium (b).\n\n\n\nTo quantify this tendency, we can count the number of overall subsystem-bath configurations \\((\\boldsymbol{s},\\tilde{\\boldsymbol{s}})\\) with subsystem energy \\(E\\). As in Eq. 3, there are \\(\\Omega(E)\\) microstates of the subsystem \\(\\boldsymbol{s}\\) all share the same energy \\(E\\). We refer to this collection of microstates as a macrostate of the subsystem with energy \\(E\\). Figure 3 illustrates this grouping of subsystem microstates into macrostates by energy. Macrostates that contain more microstates, those with higher \\(\\Omega(E)\\), are naturally observed more often. We can likewise construct macrostates of the thermal bath \\(\\tilde{\\boldsymbol{s}}\\) at energies \\(\\tilde{E}\\). Considering the number of ways to arrange the \\(\\tilde{E}\\) excited states among the \\(\\tilde{n}\\) sites of the bath, there are\n\n\\[\\begin{aligned}\n    \\tilde{\\Omega}(\\tilde{E}) = \\binom{\\tilde{n}}{\\tilde{E}}\\end{aligned}\\]\n\nmicrostates in the bath macrostate of energy \\(\\tilde{E}\\). In many settings the full microstate is not observable, for instance the precise position and velocity of each molecule of a gas. In these settings the macrostate summarizes the pieces of physically relevant information that can be observed, such as the energy or pressure.\n\n\n\n\n\n\nFigure 3: Example microstates of the subsystem \\(\\boldsymbol{s}\\) grouped into macrostates by energy \\(E\\). Only one microstate has \\(E = 0\\) while five microstates share \\(E = 1\\), meaning that the higher energy macrostate has higher entropy \\(S(E)\\). Although each microstate is equally likely, the higher entropy macrostate is more likely to be observed.\n\n\n\nSince the subsystem macrostate has \\(\\Omega(E)\\) microstates and the bath macrostate has \\(\\tilde{\\Omega}(\\tilde{E})\\), there are \\(\\Omega(E)\\tilde{\\Omega}(\\tilde{E})\\) unique pairs \\((\\boldsymbol{s},\\tilde{\\boldsymbol{s}})\\) with subsystem energy \\(E\\) and bath energy \\(\\tilde{E} = E_T - E\\). Since each pair is equally likely to appear in the overall microcanonical ensemble, we thus observe subsystem energy \\(E\\) with probability\n\n\\[\\begin{aligned}\n    P(E|E_T) &\\propto \\Omega(E)\\tilde{\\Omega}(\\tilde{E}) \\nonumber \\\\\n    &\\propto \\Omega(E) \\tilde{\\Omega}(E_T - E). \\end{aligned}\\]\n\nThese macrostate multiplicities can also be written using macrostate entropy, defined as the logarithm2\n\n\\[\\begin{aligned}\n    S(E) = \\log \\Omega(E) = \\log \\binom{n}{E}, \\text{ or }\\, \\tilde{S}(\\tilde{E}) = \\log \\tilde{\\Omega}(\\tilde{E}) = \\log \\binom{\\tilde{n}}{\\tilde{E}}. \\end{aligned} \\tag{6}\\]\n\nThe energy distribution then depends on the overall entropy \\(S(E) + \\tilde{S}(\\tilde{E})\\) as\n\n\\[\\begin{aligned}\n    P(E|E_T) \\propto e^{S(E) + \\tilde{S}(\\tilde{E})}. \\end{aligned} \\tag{7}\\]\n\nConfigurations with higher total entropy are therefore more likely to appear, as calculated in Figure 2. This tendency to observe higher entropy states is the content of the 2nd law of thermodynamics: that the entropy of the universe cannot decrease. On such large scales differences in entropy are large, and the probability Eq. 7 approaches a certainty, a law of physics."
  },
  {
    "objectID": "notes/2025/statistical_physics.html#the-canonical-ensemble",
    "href": "notes/2025/statistical_physics.html#the-canonical-ensemble",
    "title": "Statistical physics",
    "section": "The canonical ensemble",
    "text": "The canonical ensemble\nWe can similarly compute the distribution of subsystem configurations \\(\\boldsymbol{s}\\), not just of its energy \\(E\\). If we fix the subsystem microstate, valid pairs \\((\\boldsymbol{s},\\tilde{\\boldsymbol{s}})\\) correspond to bath microstates \\(\\tilde{\\boldsymbol{s}}\\) among the \\(\\tilde{\\Omega}(\\tilde{E})\\) configurations of the remaining energy. As each pair is equally probable, the subsystem is distributed as\n\n\\[\\begin{aligned}\n    P(\\boldsymbol{s}|E_T) &\\propto \\tilde{\\Omega}(\\tilde{E}) \\nonumber \\\\\n    &\\propto e^{\\tilde{S}(\\tilde{E})}. \\end{aligned} \\tag{8}\\]\n\nUnlike the uniform, microcanonical ensemble of the subsystem, certain configurations are now more or less likely based on the entropy of the surrounding bath.\nIn this picture the canonical ensemble is defined by the limit \\(\\tilde{n} \\rightarrow \\infty\\) of a large thermal bath that can effectively absorb subsystem fluctuations. We further fix the average energy \\(p = \\tilde{E}/\\tilde{n}\\) of the thermal bath in this limit, which corresponds to a fixed density of excited states. If we Stirling approximate the binomial coefficient Eq. 6, we find the bath’s entropy \\(\\tilde{S}(\\tilde{E})\\) is proportional to its energy (up to a constant \\(C\\)) as\n\n\\[\\begin{aligned}\n    \\tilde{S}(\\tilde{E}) = \\beta \\tilde{E} + C, \\quad \\beta = \\log \\frac{1-p}{p}. \\end{aligned} \\tag{9}\\]\n\nThis constant of proportionality \\(\\beta\\) is known as the inverse temperature and relates changes in the bath energy \\(\\tilde{E}\\) to changes of entropy \\(\\tilde{S}\\) as\n\n\\[\\begin{aligned}\n    \\frac{\\partial \\tilde{S}}{\\partial \\tilde{E}} = \\beta = \\frac{1}{T}, \\end{aligned} \\tag{10}\\]\n\naligning with the usual thermodynamic definition3 of the temperature \\(T\\). We will typically assume this temperature is positive and so added energy increases the entropy, although negative temperatures are possible in certain cases such as population inversion in laser physics or the coin flips when \\(p &gt; 0.5\\).\nComparing to Eq. 8, we then see that the probability of observing any given subsystem \\(\\boldsymbol{s}\\) in this canonical ensemble is\n\n\\[\\begin{aligned}\n    P(\\boldsymbol{s}|\\beta) &\\propto e^{\\beta \\tilde{E}}  \\propto e^{-\\beta E} \\nonumber \\\\\n    &\\propto e^{-\\beta H(\\boldsymbol{s})},\\end{aligned}\\]\n\nwhere we have used that the total energy \\(E_T = E + \\tilde{E}\\) is conserved. In fact, by similar arguments any system \\(\\boldsymbol{s}\\) in thermal equilibrium with a large bath at inverse temperature \\(\\beta\\) follows this same Boltzmann distribution (or \"Gibbs distribution\") \\(P(\\boldsymbol{s}) \\propto e^{-\\beta H(\\boldsymbol{s})}\\), which may itself taken as the definition of the canonical ensemble of \\(\\boldsymbol{s}\\). When \\(T = 0\\), \\(\\beta \\rightarrow \\infty\\), the subsystem is stuck in its ground state with the smallest possible energy \\(H(\\boldsymbol{s})\\). This aligns with the usual tendency of a physical system towards smaller energy, as a ball rolls down a hill. As the temperature increases and \\(T \\rightarrow \\infty, \\beta = 0\\), however, the Boltzmann distribution becomes uniform and every subsystem microstate is equally likely independent of its energy.\nAt finite \\(\\beta &gt; 0\\) between these extremes, the typical thermal configuration may not be the ground state, even though that is the single most likely microstate. Rather, the subsystem is likely to be found in some other macrostate with higher entropy \\(S(E)\\) as seen in Figure 2. Although each individual microstate of this macrostate has smaller probability than the ground state, their greater number makes their macrostate collectively more likely to be observed. This effect can be quantified by revisiting Eq. 7 for the distribution of energy \\(E\\). Since the large bath’s entropy is proportional to its energy we have\n\n\\[\\begin{aligned}\n    P(E|E_T) &\\propto e^{S(E) + \\tilde{S}(\\tilde{E})} \\propto e^{S(E) + \\beta \\tilde{E}} \\nonumber \\\\\n            &\\propto e^{S(E) - \\beta E} \\propto e^{-\\beta F(E)}\\end{aligned}\\]\n\nwhere we have defined the free energy\n\n\\[\\begin{aligned}\n    F(E) = E - T S(E).\\end{aligned}\\]\n\nThe most likely energy \\(E\\) to be observed is therefore the minimum of this free energy. Depending on the temperature \\(T\\), this may no longer be the ground state energy due to the influence of the entropy \\(S(E)\\).\nTo complete the description of the canonical ensemble, we normalize the Boltzmann distribution as\n\n\\[\\begin{aligned}\n    P(\\boldsymbol{s}|\\beta) = \\frac{1}{Z(\\beta)}e^{-\\beta H(\\boldsymbol{s})}, \\quad Z(\\beta) = \\sum_{\\boldsymbol{s}} e^{-\\beta H(\\boldsymbol{s})} \\end{aligned} \\tag{11}\\]\n\nwith the partition function \\(Z(\\beta)\\), a quantity which tells us much about the system in its own right. For example, its logarithmic derivative\n\n\\[\\begin{aligned}\n    - \\partial_\\beta \\log Z(\\beta) &= - \\frac{1}{Z(\\beta)} \\partial_\\beta \\sum_{\\boldsymbol{s}} e^{-\\beta H(\\boldsymbol{s})} \\nonumber \\\\\n    &= \\frac{1}{Z(\\beta)} \\sum_{\\boldsymbol{s}} H(\\boldsymbol{s}) e^{-\\beta H(\\boldsymbol{s})} \\nonumber \\\\\n    &= \\langle E \\rangle \\end{aligned} \\tag{12}\\]\n\nyields the average energy \\(\\langle E \\rangle\\) under the Boltzmann distribution."
  },
  {
    "objectID": "notes/2025/statistical_physics.html#physics-and-models",
    "href": "notes/2025/statistical_physics.html#physics-and-models",
    "title": "Statistical physics",
    "section": "Physics and models",
    "text": "Physics and models\nIf we normalize the Boltzmann distribution of our coin flip system with the partition function \\(Z(\\beta) = (1-p)^{-n}\\), we obtain\n\n\\[\\begin{aligned}\n    P(\\boldsymbol{s}|p) &= \\frac{1}{Z(\\beta)} e^{-\\beta H(\\boldsymbol{s})} \\nonumber \\\\\n    &= \\frac{1}{(1 - p)^{-n}} e^{-\\log \\left(\\frac{1-p}{p}\\right) \\, n_H} \\nonumber \\\\\n    &= p^{n_H}(1-p)^{n - n_H} \\end{aligned} \\tag{13}\\]\n\nwhich we recognize as the model likelihood Eq.  1 for biased coin flips with probability \\(p\\). In this correspondence, we can check that the average energy relation Eq. 12 of the physical system indeed recovers the expected number of heads\n\n\\[\\begin{aligned}\n    \\langle E \\rangle = -\\partial_\\beta \\log Z(\\beta) = p n.\\end{aligned}\\]\n\nUnlike the microcanonical ensemble, the number of observed heads \\(n_H = E\\) can now thermally fluctuate about this expectation via exchange with its surroundings in a manner that exactly reproduces independent coin flips.\nThis example motivates us to draw a broader analogy between statistics and physics. In Eq. 13 we described a physical system that reproduces the biased coin flip likelihood, the same distribution of flip outcomes given a fixed parameter \\(p\\). Often, however, we are interested in the reverse inference problem of understanding the space of likely parameter values \\(p\\) given a particular observation. Suppose we have a generic model \\(P(\\boldsymbol{x}|\\boldsymbol{\\theta})\\) of data \\(\\boldsymbol{x}\\) with parameters \\(\\boldsymbol{\\theta}\\). As discussed here, the posterior distribution of parameters inferred from data \\(\\boldsymbol{x}\\) is by Bayes’ law\n\n\\[\\begin{aligned}\n    P(\\boldsymbol{\\theta}|\\boldsymbol{x}) &= \\frac{P(\\boldsymbol{x}|\\boldsymbol{\\theta})P(\\boldsymbol{\\theta})}{P(\\boldsymbol{x})}.\\end{aligned}\\]\n\nFor a fixed observation \\(\\boldsymbol{x}\\), the corresponding physical system is defined by the Hamiltonian\n\n\\[\\begin{aligned}\n    H(\\boldsymbol{\\theta}) = -\\log P(\\boldsymbol{x}|\\boldsymbol{\\theta}) \\end{aligned} \\tag{14}\\]\n\nover the configuration space of possible parameters \\(\\boldsymbol{\\theta}\\). If we assume a uniform prior \\(P(\\boldsymbol{\\theta}) = 1\\), the posterior distribution over parameters is proportional to the Boltzmann distribution of this system at unit temperature \\(\\beta = 1\\)\n\n\\[\\begin{aligned}\n    P(\\boldsymbol{\\theta}|\\boldsymbol{x}) \\propto P(\\boldsymbol{x}|\\boldsymbol{\\theta}) \\propto e^{-H(\\boldsymbol{\\theta})}.\\end{aligned}\\]\n\nIn fact, if we normalize the Boltzmann distribution as\n\n\\[\\begin{aligned}\n    P(\\boldsymbol{\\theta}|\\boldsymbol{x}) = \\frac{1}{Z(1)} e^{-H(\\boldsymbol{\\theta})},\\end{aligned}\\]\n\nthe partition function \\(Z(1)\\) is equal to the Bayesian evidence\n\n\\[\\begin{aligned}\n    Z(1) = \\sum_{\\boldsymbol{\\theta}} e^{-H(\\boldsymbol{\\theta})} = \\sum_{\\boldsymbol{\\theta}} P(\\boldsymbol{x}|\\boldsymbol{\\theta}) P(\\boldsymbol{\\theta}) = P(\\boldsymbol{x}).\\end{aligned}\\]\n\nThrough this correspondence we can explore the posterior distribution of any Bayesian model by simulating the behavior of the analogous physical system.\nThis equivalence highlights a subtle philosophical difference between the physical perspective and the common statistical view of these problems. In statistics, particularly in frequentist treatments, we often consider and report the single best-fit parameter of a model that maximizes \\(P(\\boldsymbol{\\theta}|\\boldsymbol{x})\\). Physically, this is akin to identifying the ground state configuration with the lowest energy \\(H(\\boldsymbol{\\theta})\\).\nYet in physics, we are usually more interested in the typical behavior of the system across the entire thermal ensemble rather than the nature of the single most probable microstate. As shown in Figure 2, there may be many other less likely yet more entropic configurations that dominate the overall distribution when taken together. In common glass, for example, although the ground state would be an ordered crystalline structure, thermal fluctuations make the typical configuration amorphous, giving the material its signature uniform transparency. Keeping with this perspective, in this work we will consider the full posterior distribution when possible to give a comprehensive picture of system behavior."
  },
  {
    "objectID": "notes/2025/statistical_physics.html#markov-chain-monte-carlo",
    "href": "notes/2025/statistical_physics.html#markov-chain-monte-carlo",
    "title": "Statistical physics",
    "section": "Markov Chain Monte Carlo",
    "text": "Markov Chain Monte Carlo\nFor this purpose we employ Markov Chain Monte Carlo (MCMC) methods, a common technique to simulate generic physical systems and so to sample from generic probability distributions. This strategy performs a weighted random walk over configuration space, analogous to thermalization and dispersion in a real system. Returning to our coin flip example, we can consider how in Figure 2 the subsystem dynamically evolves. If our subsystem \\(\\boldsymbol{s}\\) begins in a the low entropy ground state (a), through the jostling of the coins the system will naturally tend towards a typical, high entropy configuration (b). Yet physically this transition is not instantaneous. For instance, gas molecules kinetically bump into each other and gradually disperse throughout their enclosure. We use Markov chain methods to simulate coin flip thermalization as a process that occurs one coin flip at a time.\nA Markov chain walks through configuration space in discrete increments of time. At each step the Markov chain transitions from one state at time \\(t\\) to another state at slightly later time \\(t + \\Delta t\\). This is a random walk, where a move from state \\(\\boldsymbol{s}\\) to \\(\\boldsymbol{s}'\\) is made with probability \\(P(\\boldsymbol{s} \\rightarrow \\boldsymbol{s}')\\). Under mild conditions on the transition probabilities, this Markov chain will converge to some equilibrium distribution \\(P(\\boldsymbol{s})\\). If the states of the Markov chain are distributed according the equilibrium \\(P(\\boldsymbol{s})\\), the final \\(\\boldsymbol{s}'\\) after the chain move \\(\\boldsymbol{s} \\rightarrow \\boldsymbol{s}'\\) will by definition share the equilibrium distribution\n\n\\[\\begin{aligned}\n    P(\\boldsymbol{s}') = \\sum_{\\boldsymbol{s}}P(\\boldsymbol{s})P(\\boldsymbol{s} \\rightarrow \\boldsymbol{s}'). \\end{aligned} \\tag{15}\\]\n\nWe are then left with the problem of constructing a Markov chain which has the particular equilibrium distribution \\(P(\\boldsymbol{s})\\) we are interested in.\nA simple and ubiquitous way to establish such a chain is known as the Metropolis-Hastings algorithm. Each step in this Markov chain consists of two parts. First, a move to a new state is proposed given the current state. Second, that move is either accepted and the chain moves to the new state, or it is rejected and the chain remains at its current state. If we propose moves with transition probabilities \\(P_{\\text{prop}}(\\boldsymbol{s} \\rightarrow \\boldsymbol{s}')\\), we accept each proposal with probability\n\n\\[\\begin{aligned}\n    P_{\\text{acc}}(\\boldsymbol{s} \\rightarrow \\boldsymbol{s}') = \\text{min}\\left(1, \\frac{P(\\boldsymbol{s}')P_{\\text{prop}}(\\boldsymbol{s}' \\rightarrow \\boldsymbol{s})}{P(\\boldsymbol{s})P_{\\text{prop}}(\\boldsymbol{s} \\rightarrow \\boldsymbol{s}')}\\right).\\end{aligned}\\]\n\nWe will often use symmetric proposals where \\(P_{\\text{prop}}(\\boldsymbol{s} \\rightarrow \\boldsymbol{s}') = P_{\\text{prop}}(\\boldsymbol{s}' \\rightarrow \\boldsymbol{s})\\), for which this acceptance probability simplifies to\n\n\\[\\begin{aligned}\n    P_{\\text{acc}}(\\boldsymbol{s} \\rightarrow \\boldsymbol{s}') = \\text{min}\\left(1, \\frac{P(\\boldsymbol{s}')}{P(\\boldsymbol{s})}\\right) = \\text{min}\\left(1, e^{\\beta(H(\\boldsymbol{s}) - H(\\boldsymbol{s}'))}\\right).\\end{aligned}\\]\n\nIn terms of the system energy \\(H(\\boldsymbol{s})\\), we see that the algorithm will always accept changes that decrease the energy, and occasionally moves that increase it, an emulation of how the real system evolves.\nFrom this two-step process the overall Markov chain transition probabilities are then distinguished by if they increase or decrease the probability as\n\n\\[\\begin{aligned}\n    P(\\boldsymbol{s} \\rightarrow \\boldsymbol{s}') = \\begin{cases}\n        P_{\\text{prop}}(\\boldsymbol{s} \\rightarrow \\boldsymbol{s}') & P(\\boldsymbol{s}') \\geq P(\\boldsymbol{s}), \\boldsymbol{s} \\not= \\boldsymbol{s}'\\\\\n        P_{\\text{prop}}(\\boldsymbol{s} \\rightarrow \\boldsymbol{s}')\\frac{P(\\boldsymbol{s}')}{P(\\boldsymbol{s})} & P(\\boldsymbol{s}') &lt; P(\\boldsymbol{s})\\\\\n        P_{\\text{prop}}(\\boldsymbol{s} \\rightarrow \\boldsymbol{s}) + \\sum_{\\boldsymbol{s}' &lt; \\boldsymbol{s}} P_{\\text{prop}}(\\boldsymbol{s} \\rightarrow \\boldsymbol{s}') \\left[1 - \\frac{P(\\boldsymbol{s}')}{P(\\boldsymbol{s})}\\right]& \\boldsymbol{s}' = \\boldsymbol{s}\n    \\end{cases}\\end{aligned}\\]\n\nwhere sum in the case \\(\\boldsymbol{s}' = \\boldsymbol{s}\\) accounts for all the rejected proposals to states \\(\\boldsymbol{s}'\\) with probability \\(P(\\boldsymbol{s}') &lt; P(\\boldsymbol{s})\\). Summing over these cases, now of states \\(\\boldsymbol{s}\\) that can produce a state \\(\\boldsymbol{s}'\\), we can check the stability condition Eq. 15 as\n\n\\[\\begin{aligned}\n    \\sum_{\\boldsymbol{s}} P(\\boldsymbol{s})P(\\boldsymbol{s} \\rightarrow \\boldsymbol{s}') &= \\sum_{\\boldsymbol{s} &lt; \\boldsymbol{s}'} P(\\boldsymbol{s}) P_{\\text{prop}}(\\boldsymbol{s} \\rightarrow \\boldsymbol{s}') + \\sum_{\\boldsymbol{s} &gt; \\boldsymbol{s}'} P(\\boldsymbol{s}) \\frac{P(\\boldsymbol{s}')}{P(\\boldsymbol{s})}P_{\\text{prop}}(\\boldsymbol{s} \\rightarrow \\boldsymbol{s}') \\nonumber\\\\\n    &+ P(\\boldsymbol{s}')P_{\\text{prop}}(\\boldsymbol{s}' \\rightarrow \\boldsymbol{s}') + P(\\boldsymbol{s}') \\sum_{\\boldsymbol{s} &gt; \\boldsymbol{s}'} P_{\\text{prop}}(\\boldsymbol{s}' \\rightarrow \\boldsymbol{s})\\left[1 - \\frac{P(\\boldsymbol{s})}{P(\\boldsymbol{s}')}\\right] \\nonumber\\\\\n    &= P(\\boldsymbol{s}')\\sum_{\\boldsymbol{s}} P_{\\text{prop}}(\\boldsymbol{s} \\rightarrow \\boldsymbol{s}') = P(\\boldsymbol{s}').\\end{aligned}\\]\n\nTherefore the desired distribution is a fixed point of the Metropolis-Hastings algorithm, regardless of the choice of proposals \\(P_{\\text{prop}}(\\boldsymbol{s} \\rightarrow \\boldsymbol{s}')\\). However, being a Markov chain, there is still a clear correlation between subsequent steps in the random walk. Only after a typical number of Markov chain known as the mixing time are the samples meaningfully independent. Therefore to efficiently obtain independent samples from the distribution, to for example compute expectations, the mixing time should be as small as possible. The form these proposals take can considerably impact the mixing time, and clever choices are often needed to make Monte Carlo methods tractable.\nA common choice of proposal for discrete settings like this is to consider single site flips where we choose a random site \\(i\\) and flip it from heads to tails, \\(s_i = 1 \\mapsto 0\\) or vice versa. An advantage to this local change is that the resulting chance in the probability (or energy) is small and therefore likely to be accepted. If a entirely new global configuration is drawn uniformly at random, it likely has a much lower probability (or higher energy) than the current sample, and so will likely be rejected and waste an iteration of the algorithm.\n\n\n\n\n\n\nFigure 4: Trajectory of the Metropolis-Hastings MCMC algorithm to thermalize a system of 100 coin flips. The system begins in the ground state where \\(n_H = 0\\). The average value \\(n_H = 40\\) is highlighted. Once the Markov chain has thermalized, the samples are indicative of this average value.\n\n\n\nIf we apply this to our simple coin flipping example, Figure 4 shows how the number of flips changes over the course of the Metropolis-Hastings algorithm. We can observe that although the system starts in a configuration not particularly representative of the equilibrium distribution, after many iterations the Markov chain thermalizes the state. Although this is a fairly simple distribution, one we could sample directly, the flexibility of MCMC allows us apply it to far more complex distributions to draw inferences. In doing this, however, we must be careful to ensure that the Markov chain has adequately converged. If samples are interpreted too early in the algorithm, results will be skewed by the initial state, as in the initial samples of Figure 4.\nThese Monte Carlo methods are very powerful, and are the workhorse behind much of Bayesian inference. In order to apply these methods to their fullest, we can further augment the Metropolis-Hastings algorithm by performing parallel tempering to reduce the mixing time of the Markov chains and thermodynamic integration to compute the Bayesian evidence in a manner analogous to how such methods are used to compute the physical free energy."
  },
  {
    "objectID": "notes/2025/statistical_physics.html#footnotes",
    "href": "notes/2025/statistical_physics.html#footnotes",
    "title": "Statistical physics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe two values of an Ising model spin are typically denoted as \\(s_i = 1\\) for the \"up\" state and \\(s_i = -1\\) for the \"down\" state, in line with the magnetic dipole moments they physically represent, although the choice of basis does not alter the underlying physics.↩︎\nMore generally defined as \\(S = k_B \\log \\Omega\\) where \\(k_B \\approx 1.38 \\text{ J}/\\text{K}\\) is the Boltzmann constant. The units of this constant are due to the thermodynamic relation Eq. 10.↩︎\nThis definition ensures that if two thermal baths at temperatures \\(T_1 &gt; T_2 &gt; 0\\) are brought into contact energy will flow from the higher temperature bath to the lower temperature bath to increase the overall entropy, in keeping with the 2nd law of thermodynamics.↩︎"
  },
  {
    "objectID": "notes/2025/information_theory.html",
    "href": "notes/2025/information_theory.html",
    "title": "Information theory",
    "section": "",
    "text": "In this note we informally discuss the foundations of information theory from Shannon entropy to Kolmogorov complexity. These concepts allow us to reckon with the inherent complexity of the data observed in a general sense, underpinning much of my research. The presentation is adapted from my PhD thesis, alongside reviews of statistical inference and physics."
  },
  {
    "objectID": "notes/2025/information_theory.html#encoding",
    "href": "notes/2025/information_theory.html#encoding",
    "title": "Information theory",
    "section": "Encoding",
    "text": "Encoding\nCentral to information theory is a thought experiment aiming to encode a message as efficiently as possible. Suppose we would like to transmit to another party the results of 10 coin flips as the message \"HHTTHTTHTT.\" Suppose further that we are restricted in this communication to use a binary channel which can only send a binary sequence of 0’s and 1’s of our choosing. We would then like the receiver on the other end of the channel to be able to decode our binary transmission back into our original message.\nFor example, we can encode these coin flips as the binary string \"1100100100\" where the digit \"1\" represents heads and \"0\" represents tails. This correspondence between meanings and binary strings is known as a codebook. So long as we and the receiver agree on the nature of the encoding, the receiver can use the codebook to decode the binary string back into the original message of outcomes. To determine the efficiency of our transmission we measure the length of our binary string in bits. In this case our encoding used 10 digits (bits) to transmit the message, one for each flip. A schematic of this encoding framework is given in Figure 1.\n\n\n\n\n\n\nFigure 1: Encoding coin flips as a binary string\n\n\n\nThis association of digits to outcomes is quite a natural encoding of two-sided coin flips. In a more general setting, however, we will need to be more creative in our transmission. Suppose we would instead like to send the message \"ELEVENELVES\" as a binary string. Since this string contains 5 distinct characters, we can no longer encode the message by assigning each character its own binary digit. Some characters must be represented by a codeword of multiple binary digits. If we are not careful, however, this can render our message ambiguous. If we assign \"E\" to the codeword \"0\" and \"L\" to the codeword \"00,\" the binary message \"00\" could decode into either \"EE\" or \"L.\"\nTo avoid this polysemy, we can ensure that our transmission is uniquely decodable by using a prefix-free (or \"instantaneous\") code. If no codeword in our codebook is a prefix of another codeword, as the receiver reads the message from left to right the divisions between the codewords will always be clear. Our earlier example was not prefix-free as the codeword \"0\" is a prefix of the codeword \"00,\" leading to the double meaning.\nAny such prefix-free binary code can be usefully represented as a binary tree whose leaves each correspond to a character (or \"symbol\") being transmitted. The codeword associated to each symbol is then represented by the path from the root of the tree down to that symbol. Figure 2a shows an example of such a tree used to encode the five characters. Following the tree paths this \"balanced\" encoding represents the characters {\"E\", \"L\", \"V\", \"N\", \"S\"} with the codewords {\"00\", \"01\", \"10\", \"110\", \"111\"} respectively. With this codebook we can then encode the phrase \"ELEVENELVES\" into pictured binary string of length 24 bits and ensure that it uniquely decodes back into our desired dispatch.\n\n\n\n\n\n\nFigure 2: Examples of an (a) balanced code and (b) optimized Huffman code to convert the phrase \"ELEVENELVES\" into a binary string. The string associated to each letter is denoted by the path from the top of the tree down to the appropriate node. The Huffman code is able to produce a shorter overall message than the balanced code by representing the common letter \"E\" with a short string \"0\" despite representing the uncommon letters \"N\" and \"S\" with longer strings."
  },
  {
    "objectID": "notes/2025/information_theory.html#encoding-efficiently",
    "href": "notes/2025/information_theory.html#encoding-efficiently",
    "title": "Information theory",
    "section": "Encoding efficiently",
    "text": "Encoding efficiently\nNow, a key goal of information theory is not only to successfully transmit a message but also to do so using as few bits as possible. This objective can be seen as a formalization of Occam’s razor, the scientific principle that favors the simplest possible answer to a question. In this analogy, transmitting our binary string effectively \"explains\" to the receiver the data we have observed, making a shorter transmission a more succinct explanation. If we understand predictable patterns in our observations, we can exploit them to construct a more efficient encodings. There is a fundamental duality between compression and modeling, as in this context, to compress is to understand.\nIn this spirit we can look for patterns in our message to try and come up with a clever way to shorten our encoding. The phrase \"ELEVENELVES\" has a rather lopsided distribution of characters at 5 E’s, 2 L’s, 2 V’s, 1 N, and 1 S. Some of our codewords are therefore being used in the transmission much more frequently than others. If symbol \\(r = 1,...,q\\) of the \\(q\\) symbols appears \\(n_r\\) times in our message as a codeword of length \\(\\ell_r\\), the total message length is\n\n\\[\\begin{aligned}\n    \\sum_{r=1}^q n_r \\ell_r. \\end{aligned} \\tag{1}\\]\n\nTo shorten the overall message we would therefore like the codewords to be as short as possible and to prioritize shortening frequently occurring symbols. In our original encoding the symbol \"E\" is transmitted with the codeword \"00\" at a cost of two bits apiece. Since \"E\" appears so frequently in the message it may be wise to instead represent it with a shorter codeword like \"0\" and save 5 bits in our total transmission.\nYet this choice has a cost. If we assign \"0\" to represent \"E\" none of the other four characters can be represented with a codeword that begins with a \"1\" or else the code would no longer be prefix-free. When shortening one codeword we must necessarily lengthen other codewords. This tradeoff is the content of Kraft’s inequality, which states that the codeword lengths of any prefix-free code must satisfy\n\n\\[\\begin{aligned}\n    \\sum_{r=1}^q 2^{-\\ell_r} \\leq 1, \\end{aligned} \\tag{2}\\]\n\nconsidering the fraction of the binary tree each codeword occupies. Given the frequencies \\(n_r\\) at which each symbol appears, we would like to select codeword lengths that minimize the total message length Eq. 1 subject to the prefix-free constraint Eq. 2.\nHuffman codes strike this balance and provably minimize the message length by assigning short codewords to frequent symbols while saturating Kraft’s inequality. Figure 2b contains an example of a Huffman code for our application. The code shortens the codeword for \"E\" from 2 to 1 bit while lengthening the codewords for \"N\" and \"S\" from 3 to 4 bits. Since \"E\" appears much more frequently than \"N\" and \"S,\" this change shortens the overall message from 24 to 23 bits. By adapting the encoding to the nature of the data, the Huffman code achieves a more parsimonious representation of the message.\nMore generally given a frequency distribution \\(\\{n_r\\}\\) of symbols one can always construct a Huffman code with a simple recursive algorithm. The resulting optimal codeword lengths \\(\\{\\ell_r\\}\\) follow a predictable pattern. If a symbol appears at a fraction \\(p_r = \\frac{n_r}{n}\\) among the \\(n\\) total symbols, the length of its associated codeword satisfies\n\n\\[\\begin{aligned}\n      -\\log_2(p_r) \\leq \\ell_r &lt; -\\log_2(p_r) + 1.\\end{aligned}\\]\n\nFrequent symbols with high probability \\(p_r\\) are thus assigned small codewords as \\(-\\log_2(p_r)\\) is small while infrequent symbols use longer codewords. In our \"ELEVENELVES\" example, the character \"E\" appears with probability \\(p = 5/11\\), and is so assigned a string of length \\(1 \\approx \\log_2(11/5)\\) while the character \"S\" appears at the ratio \\(1/11\\) and is encoded using \\(4 \\approx \\log_2(11)\\) bits."
  },
  {
    "objectID": "notes/2025/information_theory.html#shannon-entropy",
    "href": "notes/2025/information_theory.html#shannon-entropy",
    "title": "Information theory",
    "section": "Shannon entropy",
    "text": "Shannon entropy\nIn most contexts we consider, symbol probabilities are small and codeword lengths are long. In this regime we can approximate lengths as \\(\\ell_r = -\\log_2(p_r)\\), which would in fact be the optimal choices if the lengths could be non-integral numbers of bits. Using these optimal codeword lengths, the minimum message length per symbol is\n\n\\[\\begin{aligned}\n    S[\\{p_r\\}] = \\frac{1}{n} \\sum_{r=1}^q n_r \\ell_r = -\\sum_{r=1}^q p_r \\log_2 p_r, \\end{aligned} \\tag{3}\\]\n\nknown as the Shannon entropy (or simply \"entropy\") of the distribution \\(\\{p_r\\}\\). For continuous distributions \\(P(\\boldsymbol{x})\\) this entropy generalizes to\n\n\\[\\begin{aligned}\nS[P] = -\\int P(\\boldsymbol{x}) \\log_2 P(\\boldsymbol{x}) d\\boldsymbol{x}. \\end{aligned} \\tag{4}\\]\n\nBy providing an information theoretic lower bound on transmission, the Shannon entropy captures the inherent information content of a probability distribution that no amount of clever encoding tricks can overcome.\nWe can make contact between this information-theoretic entropy and the physical entropy described in this note. There the microcanonical ensemble is the uniform distribution \\(P(\\boldsymbol{s}) = \\frac{1}{\\Omega}\\) over all possible configurations that conserve the total energy. The Shannon entropy Eq. 3 then agrees which the typical microcanonical entropy \\(S = \\log \\Omega\\). From this perspective, a macrostate with high physical entropy is one where a large amount of information is required to specify which of the many possible microstates it represents.\nWe also note that the uniform distribution has the highest entropy among all possible distributions \\(\\{p_r\\}\\) on \\(q\\) objects. By the convexity of the logarithm,\n\n\\[\\begin{aligned}\n    S[\\{p_r\\}] = -\\sum_{r=1}^q p_r \\log p_r \\leq -\\sum_{r=1}^q \\frac{1}{q} \\log \\left(\\frac{1}{q}\\right) = \\log q.\\end{aligned}\\]\n\nThis observation gives another motivation for the microcanonical ensemble: the maximum-entropy distribution over possible configurations. Since the entropy measures how structured, how compressible, a probability distribution is, this maximum-entropy distribution is structureless and maximally agnostic: a reasonable properties of an equilibrium distribution where any initial structure is thermalized away.\nThe canonical ensemble \\(P(\\boldsymbol{s}) \\propto e^{-\\beta H(\\boldsymbol{s})}\\) can likewise be motivated as a maximum entropy distribution with a given average energy, which determines the choice of \\(\\beta\\). When designing priors for Bayesian inference, we will frequently appeal to this minimally assumptive principle and choose maximum-entropy priors subject to certain constraints we expect the system to provide. For example, a Gaussian distribution can be motivated as the maximum entropy distribution of a real random variable of a fixed mean and variance. A nice list of common statistical distribution and their maximum-entropy motivations can be found on Wikipedia."
  },
  {
    "objectID": "notes/2025/information_theory.html#misspecification-and-kl-divergence",
    "href": "notes/2025/information_theory.html#misspecification-and-kl-divergence",
    "title": "Information theory",
    "section": "Misspecification and KL-divergence",
    "text": "Misspecification and KL-divergence\nReturning to encodings, to obtain the entropy we had considered the total message length of a Huffman code optimized to send that particular message. If we believe that the symbols will be distributed with probabilities \\(\\{q_r\\}\\) we should optimize our Huffman code accordingly to have code lengths \\(\\ell_r = -\\log q_r\\). However in many contexts we do not a priori know what distribution of symbols to expect. We may assume a distribution \\(\\{q_r\\}\\) that is not borne out in practice. If the symbols we must transmit have true probabilities \\(\\{p_r\\}\\) the average code length becomes\n\n\\[\\begin{aligned}\n    \\sum_r p_r (-\\log q_r).\\end{aligned}\\]\n\nHad we used code lengths \\(-\\log p_r\\) attuned to the true distribution, this would give the Shannon lower bound. Since the our encoding is misspecified it will instead require a larger number of bits to transmit. The shortfall between the two, the extra cost we incur, is known as the Kullback-Leibler (KL) divergence between the true distribution \\(\\{p_r\\}\\) and our assumption \\(\\{q_r\\}\\):\n\n\\[\\begin{aligned}\n    D_{\\text{KL}}(\\{p_r\\}||\\{q_r\\}) &= \\left(\\sum_{k=1}^q p_r (-\\log q_r) \\right) - \\left(\\sum_{k=1}^q p_r (-\\log p_r) \\right) \\nonumber \\\\\n    &= \\sum_{k=1}^q p_r \\log \\frac{p_r}{q_r} \\geq 0.\\end{aligned}\\]\n\nThis story repeats when modeling data. Given a model with distribution \\(Q(\\boldsymbol{x})\\) we can write the Huffman code length (or description length) of an observation \\(\\boldsymbol{x}\\) as\n\n\\[\\begin{aligned}\n    H(\\boldsymbol{x}) = -\\log Q(\\boldsymbol{x}),\\end{aligned}\\]\n\nwhich we can compare to the relation \\(H(\\boldsymbol{x}) = - \\log P(\\boldsymbol{x})\\) between system energy and model probability discussed here. If we use this model encoding on a stream of observations whose true distribution is \\(P(\\boldsymbol{x})\\), the average description length decomposes as\n\n\\[\\begin{aligned}\n    \\sum_{\\boldsymbol{x}} P(\\boldsymbol{x}) H(\\boldsymbol{x}) &= -\\sum_{\\boldsymbol{x}} P(\\boldsymbol{x})\\log P(\\boldsymbol{x}) + \\sum_{\\boldsymbol{x}} P(\\boldsymbol{x})\\log \\frac{P(\\boldsymbol{x})}{Q(\\boldsymbol{x})} \\nonumber \\\\\n    &= \\underbrace{S[P]}_{\\text{entropy}} +  \\underbrace{D_{\\text{KL}}(P||Q)}_{\\text{cross-entropy}}\\end{aligned}\\]\n\ninto the inherent entropy of the data and the cross-entropy cost of our model misspecification. As we model the random process \\(P(\\boldsymbol{x})\\) our average description length can never fall below the entropic lower bound, but any description length above this point is evidence of the failure of our model to match reality. While it is relatively straightforward to measure this average description length in practice, deducing what fraction of it is due to the entropy or the cross-entropy is a hard problem. When comparing the average description lengths of two models on the same stream of data, however, we can confidently attribute their difference to a difference in the cross-entropies and prefer the model with the smaller average description length. This motivates the minimum description length (MDL) principle, which prefers models whose corresponding encodings across realistic data sets are as small as possible."
  },
  {
    "objectID": "notes/2025/information_theory.html#statistical-perspective",
    "href": "notes/2025/information_theory.html#statistical-perspective",
    "title": "Information theory",
    "section": "Statistical perspective",
    "text": "Statistical perspective\nAs an application, suppose that we would like to select the appropriate value of a parameter \\(\\boldsymbol{\\theta}\\) for a model \\(P(\\boldsymbol{x}|\\boldsymbol{\\theta})\\). For each choice of parameter, the corresponding model description length is simply the minus log-likelihood \\(H(\\boldsymbol{x}|\\boldsymbol{\\theta}) = -\\log P(\\boldsymbol{x}|\\boldsymbol{\\theta})\\). Choosing the model that minimizes the description length therefore amounts to finding the maximum-likelihood estimate of the parameter as\n\n\\[\\begin{aligned}\n    \\text{argmin}_{\\boldsymbol{\\theta}} H(\\boldsymbol{x}|\\boldsymbol{\\theta}) = \\text{argmax}_{\\boldsymbol{\\theta}} P(\\boldsymbol{x}|\\boldsymbol{\\theta}).\\end{aligned}\\]\n\nAs we discussed in this post, however, this maximum likelihood estimation is prone to overfitting. This approach is also problematic from an information-theoretic perspective. In our optimization of the transmission we have neglected the cost of transmitting the parameter \\(\\boldsymbol{\\theta}\\) itself. In the Bayesian context this parameter will be distributed according to a prior \\(P(\\boldsymbol{\\theta})\\) that corresponds to its own encoding\n\n\\[\\begin{aligned}\n    H(\\boldsymbol{\\theta}) = -\\log P(\\boldsymbol{\\theta}). \\end{aligned} \\tag{5}\\]\n\nIf we consider the total information cost of this now two stage process of first transmitting the parameter \\(\\boldsymbol{\\theta}\\) and then the data \\(\\boldsymbol{x}\\) given that parameter, we recover Bayesian maximum a posteriori (MAP) estimation\n\n\\[\\begin{aligned}\n    \\text{argmin}_{\\boldsymbol{\\theta}} \\left[H(\\boldsymbol{x}|\\boldsymbol{\\theta}) + H(\\boldsymbol{\\theta})\\right] &= \\text{argmax}_{\\boldsymbol{\\theta}} P(\\boldsymbol{x}|\\boldsymbol{\\theta})P(\\boldsymbol{\\theta})  \\nonumber \\\\\n    &= \\text{argmax}_{\\boldsymbol{\\theta}} P(\\boldsymbol{\\theta}|\\boldsymbol{x}). \\end{aligned} \\tag{6}\\]\n\nThe maximum likelihood and MAP estimates of a parameter often differ considerably, particularly when a relatively small amount of data is available.\nAfter the two stage transmission process of Eq. 6 we transmit to the receiver both the data of interest \\(\\boldsymbol{x}\\) and the best-fit parameter \\(\\boldsymbol{\\theta}\\) used. When assessing model performance, however, knowledge of the the parameter is often redundant to the data. We can instead holistically evaluate model performance using the Bayesian evidence, the probability that a model generates a particular data \\(\\boldsymbol{x}\\) summed over all possible parameters\n\n\\[\\begin{aligned}\n    P(\\boldsymbol{x}) = \\int P(\\boldsymbol{x}|\\boldsymbol{\\theta})P(\\boldsymbol{\\theta}).\\end{aligned}\\]\n\nThe description length of the integrated model is then\n\n\\[\\begin{aligned}\n    H(\\boldsymbol{x}) = -\\log P(\\boldsymbol{x}).\\end{aligned}\\]\n\nTherefore we can motivate model selection that chooses model with higher Bayesian evidence, as when computing Bayes factors, as selecting the model that more efficiently compresses the data integrated over its latent parameters. These connections highlight the duality between the compression and modeling of a data set."
  },
  {
    "objectID": "notes/2025/information_theory.html#kolmogorov-complexity",
    "href": "notes/2025/information_theory.html#kolmogorov-complexity",
    "title": "Information theory",
    "section": "Kolmogorov complexity",
    "text": "Kolmogorov complexity\nTo this point, we have considered the Shannon entropy of probability distributions. However, much of this thesis focuses on a subtly different notion of the complexity of objects. Shannon entropy quantifies the complexity of a probability distribution without regard to the specific objects within that distribution. For instance, suppose that we want to transmit one of two messages: the full text of Dune by Frank Hebert or It by Stephen King. While each book’s content is undoubtedly \"complex,\" our current framework would allow us to \"transmit\" them with minimal information cost. If we define an encoding where \"0\" represents the text of Dune and \"1\" stands for It, we could send a single \"0\" to transmit the entirety of Dune. This setup might suggest that the inherent information cost of Dune’s content is just one bit, which is clearly an unreasonable conclusion.\nThe problem is that we have overlooked the information cost required to establish the codebooks. If the receiver is unaware of our coding scheme, we must first communicate the full text that each binary digit corresponds to before our transmission. Once this scheme is established, we can indeed send our choice of book with a single bit repeatedly at low cost. However, the initial information cost of creating the codebook is much higher. Shannon entropy measures the information needed to transmit objects drawn from a probability distribution, not the complexity of the objects themselves.\nTo broach the information content of an object, we should instead turn to the Kolmogorov complexity. Certain objects and outcomes appear to be inherently more complicated than others. For example a sequence of coin flips \"HHHHHHHHHH\" is easy to describe as \"10 heads in a row.\" Even if the sequence was 1000 heads, the outcome would not be much more complex to describe. On the other hand, the pattern of coin flips \"HHTTHTTHTT\" we observed appears to be more complicated to describe. However, even in this case the coin flips we observed are simply the first 10 digits of \\(\\pi = 11.00100100..._2\\) in binary: a concise, if unusual, explanation. Yet if we are presented with a truly \"random\" string of coin flips, there is little hope for such an efficient description of the outcomes. The Kolmogorov complexity is meant to capture the difference between these settings and fundamentally measure how structured a given data set is.\nRoughly speaking, the Kolmogorov complexity \\(K(\\boldsymbol{x})\\) can be understood as the length of the shortest computer program that would output the object (typically string) in question. In our earlier examples, this program might be \"output 10 heads\" or \"first 10 digits of \\(\\pi_2\\)\" in pseudocode. In our earlier example of books, the full texts may be compressed with a technique like the Lempel-Ziv-Welch algorithm used in the .gif file format. The \"program\" in this case would consist of a description of the LZW algorithm, followed by the compressed file. The total program size, and so complexity will still be fairly large as some fraction of the original length of the book, but is much greater than the single bit we had used to transmit it in a probabilistic sense.\nThe \"computer program1\" in the definition of the Kolmogorov complexity is vague enough to accommodate any possible valid encoding or explanation of a string. For example, we can consider the Huffman encoding of the data \\(\\boldsymbol{x}\\) generated by a model \\(M\\). We can imagine a computer program which consists of a description of the model \\(M\\) itself, then provides the Huffman code as a binary string of length \\(H_M(\\boldsymbol{x})\\) that can be decoded with knowledge of the model. When the data set is large, we typically neglect the constant overhead required to describe the model and this framework and roughly say that an encoding of length \\(H_M(\\boldsymbol{x})\\) is possible for the data \\(\\boldsymbol{x}\\)2.\nFrom this perspective each model can be viewed as a competing encoding of the data, each of which provides an upper bound on the inherent complexity. If we have a basket of candidate models \\(M \\in \\mathcal{M}\\), we can then loosely approximate the \"true\" information cost of \\(\\boldsymbol{x}\\) as the minimum\n\n\\[\\begin{aligned}\n    K(\\boldsymbol{x}) \\sim \\text{min}_{M \\in \\mathcal{M}} H_{M}(\\boldsymbol{x}).\\end{aligned}\\]\n\nThe higher the model evidence the shorter the description length, yielding a tighter upper bound on the true cost.\nDespite this relative improvement, it is not possible to conclusively show that our approximation is particularly close to the truth. There may always be clever encoding out there that transmits the data far more efficiently than the models we consider. To show that \\(K(\\boldsymbol{x})\\) is above some value \\(n\\), we would need to check the outputs of all possible programs of length less than or equal to \\(n\\), an uncomputable task. For example, we may not recognize our initial sequence of coin flips \"HHTTHTTHTT\" as the first 10 digits of \\(\\pi = 11.00100100..._2\\) in binary, a more concise explanation that our models.\nAlthough we cannot find the perfect encoding, nor the perfect model, we can strive for a better understanding of systems and their complexity in this information-theoretic framework."
  },
  {
    "objectID": "notes/2025/information_theory.html#footnotes",
    "href": "notes/2025/information_theory.html#footnotes",
    "title": "Information theory",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMore formally a universal Turing machine.↩︎\nIn this pursuit we cannot consider models too finely attuned to a particular data set, or else we can no longer neglect the cost to specify the model itself.↩︎"
  },
  {
    "objectID": "notes/2025/statistical_inference.html#prediction-and-validation",
    "href": "notes/2025/statistical_inference.html#prediction-and-validation",
    "title": "Statistical inference",
    "section": "Prediction and validation",
    "text": "Prediction and validation\nThe Bayesian evidence offers a perspective on unsupervised machine learning tasks, where we aim to understand a data set in isolation without guidance or predefined outcomes. As the probability that the model generates a given observation, the evidence measures the quality of our model in a manner intrinsic to the data itself.\nOften, however, we may be interested in applying our model and its inferences beyond the scope of the initial data set. One application is to predict future events; for example, in a college football network of match outcomes, we might predict the winner between two teams that did not compete during the regular season. Additionally, we may validate our inferences against expert knowledge or existing context in a supervised setting, or compare the outputs of different models applied to the same data set. These practical purposes often fall under the umbrella of machine learning, which offers many methods to address these questions.\nIf we assume that the same mechanisms that generated our observed data also inform unobserved outcomes, we can leverage our model inferences to make predictions. For example, if we observe a coin and are convinced it is fair, we may predict that future flips of the coin will land heads and tails with equal probability. This extrapolation may or may not be accurate. In machine learning terminology, we initially fit the model to the \"training\" data and then assess the quality of the resulting predictions using a \"testing\" data set.\n\n\n\n\n\n\nFigure 4: Schematic of cross-validation for the coin flip example.\n\n\n\nIn our coin flip example, we may split the sequence \\(\\boldsymbol{s}\\) we observe into a training set \\(\\boldsymbol{s}^{\\text{train}}\\) of \\(n^{\\text{train}}\\) flips and testing set \\(\\boldsymbol{s}^{\\text{test}}\\) of \\(n^{\\text{test}}\\) flips. Figure 4 provides a schematic of this cross-validation set up. After fitting the model to the training data we obtain the posterior distribution of the probability \\(p\\), represented as \\(P(p|\\boldsymbol{s}^{\\text{train}})\\), which is maximized by the best fit\n\n\\[\\begin{aligned}\n    \\hat{p}_{\\text{MAP}}^{\\text{train}} = \\frac{n_H^{\\text{train}} + 20}{n^{\\text{train}} + 40}\\end{aligned}\\]\n\nas in Eq. 6. Assuming the withheld testing data \\(\\boldsymbol{s}^{\\text{test}}\\) is governed by the same parameter \\(\\hat{p}_{\\text{MAP}}^{\\text{train}}\\) as the training data, we can evaluate the likelihood Eq. 2 on the testing data\n\n\\[\\begin{aligned}\nP(\\boldsymbol{s}^{\\text{test}}|\\hat{p}_{\\text{MAP}}^{\\text{train}}) = \\left(\\frac{n_H^{\\text{train}} + 20}{n^{\\text{train}} + 40}\\right)^{n_H^{\\text{test}}}\\left(\\frac{n_T^{\\text{train}} + 20}{n^{\\text{train}} + 40}\\right)^{n_T^{\\text{test}}}. \\label{eq:log-likelihood-cross-validation}\\end{aligned}\\]\n\nThis serves as a measure of the model’s out-of-sample predictive performance.\nWhile most cross-validation tests use the single best parameter, we can instead use the full posterior distribution of possible parameters to compute the posterior predictive\n\n\\[\\begin{aligned}\n    P(\\boldsymbol{s}^{\\text{test}}|\\boldsymbol{s}^{\\text{train}}) &= \\int P(\\boldsymbol{s}^{\\text{test}}|p)P(p|\\boldsymbol{s}^{\\text{train}}) dp \\nonumber \\\\\n    &= \\frac{(n^{\\text{train}} + 41)!(n_H + 20)!(n_T + 20)!}{(n + 41)!(n_H^{\\text{train}} + 20)!(n_H^{\\text{train}} + 20)!}.\\end{aligned}\\]\n\nThis distribution is equal to the probability that the model generates the data \\(\\boldsymbol{s}^{\\text{test}}\\) conditioned on it also generating \\(\\boldsymbol{s}^{\\text{train}}\\).\nIn a cross validation context, the initial data set \\(\\boldsymbol{s}\\) is randomly split into the training and testing data sets, often at a 80/20 ratio. The predictive performance of the model is quantified using either the likelihood or posterior predictive. In keeping with the information-theoretic interpretation, we typically report the negative log likelihood or posterior predictive as\n\n\\[\\begin{aligned}\n    \\langle H(\\boldsymbol{s}^{\\text{test}}|\\hat{p}_{\\text{MAP}}^{\\text{train}})\\rangle_{\\boldsymbol{s}^{\\text{test}},\\boldsymbol{s}^{\\text{train}}} &= \\langle -\\log P(\\boldsymbol{s}^{\\text{test}}|\\hat{p}_{\\text{MAP}}^{\\text{train}})\\rangle_{\\boldsymbol{s}^{\\text{test}},\\boldsymbol{s}^{\\text{train}}},\n    \\nonumber \\\\\n    \\langle H(\\boldsymbol{s}^{\\text{test}}|\\boldsymbol{s}^{\\text{train}})\\rangle_{\\boldsymbol{s}^{\\text{test}},\\boldsymbol{s}^{\\text{train}}} &= \\langle -\\log P(\\boldsymbol{s}^{\\text{test}}|\\boldsymbol{s}^{\\text{train}})\\rangle_{\\boldsymbol{s}^{\\text{test}},\\boldsymbol{s}^{\\text{train}}}, \\label{eq:log-posterior-predictive-cross-validation}\\end{aligned}\\]\n\nwhere the results are averaged over many possible validation splits \\(\\boldsymbol{s}^{\\text{train}},\\boldsymbol{s}^{\\text{test}}\\). In practice the likelihood and posterior predictive can give different results, but we will generally prefer to use the latter to evaluate the full posterior of possible parameter values.\nThe Bayesian evidence can also be viewed as a measure of predictive performance, averaged over various data splits. We can write out our data set \\(\\boldsymbol{s}\\) as the sequence of coin flips \\(s_1,...,s_n\\). Bayesian evidence is the probability that the model generates this entire sequence. Meanwhile, the posterior predictive is the probability that the model generates some new piece of data given what it has already generated. By sampling the posterior predictive one coin flip at a time, we can therefore sequentially generate the full sequence.\nWe start by sampling the first flip \\(s_1\\), which is equally likely a priori to be heads or tails. This outcome informs the next coin flip, drawn according to the posterior predictive \\(P(s_2|s_1)\\). This repeats until the final coin is predicted using all preceding results using \\(P(s_n|s_{n-1},...,s_1)\\). By definition of the posterior predictive, the overall probability of generating any given sequence of observations must then equal the Bayesian evidence as\n\n\\[\\begin{aligned}\n    P(\\boldsymbol{s}) &= P(s_n,s_{n-1},...,s_1) \\nonumber \\\\\n    &= P(s_n|s_{n-1},...,s_1)...P(s_2|s_1)P(s_1).\\end{aligned}\\]\n\nFrom the logarithm of this equation, the description length of the data is the sum over the log-posterior-predictives at each step:\n\n\\[\\begin{aligned}\n    H(\\boldsymbol{s}) = H(s_n|s_{n-1},...,s_1) + ... + H(s_2|s_1) + H(s_1).\\end{aligned}\\]\n\nThis relationship holds regardless of the order in which the coin flips are considered. Therefore, the normalized description length is also equal to a suitably defined average\n\n\\[\\begin{aligned}\n    \\frac{1}{n}H(\\boldsymbol{s}) = \\langle H(s_i|\\boldsymbol{s}^{\\text{train}})\\rangle_{i,\\boldsymbol{s}^{\\text{train}}}\\end{aligned}\\]\n\nover all possible subsets of training data and choices of single withheld test point \\(s_i\\) (Fong and Holmes 2020).\nWe can thus use the Bayesian evidence not only as an information theoretic measure for model selection, but also as an indicator of overall predictive power. However, cross-validation results quantified using the log-likelihood ?@eq-log-likelihood-cross-validation and log-posterior-predictive ?@eq-log-posterior-predictive-cross-validation provide subtly different information about model performance, and are more common in much of the machine learning literature.\nBeyond prediction, we would often like to assess the quality of the inferred parameters directly. If we know from an artificial or empirical context that a parameter truly has a certain value, how does our inferred value compare? One way to establish such a \"true\" parameter value is in a synthetic test where we first draw a true value of the parameter \\(p^{\\text{true}}\\) from the prior \\(P(p)\\). We then sample an artificial data set \\(\\boldsymbol{s}\\) from the model likelihood \\(P(\\boldsymbol{s}|p^{\\text{true}})\\). Based solely on the resulting data \\(\\boldsymbol{s}\\), we then infer the parameter \\(p\\) and compare it to the underlying \\(p^{\\text{true}}\\).\nIn this Bayesian setting, the posterior \\(P(p|\\boldsymbol{s})\\) is by definition precisely the distribution of the parameters \\(p\\) that could have resulted in the observation \\(\\boldsymbol{s}\\). Thus, the full posterior distribution gives a complete and optimal description of the truth. Compared to this benchmark, synthetic tests provide valuable test cases to understand deviations in the inferences. For example, we can examine how inferences differ when models are misspecified and do not align with the actual generative process. Understanding this robustness is crucial when applying models to real data, where they very likely do not match the real generative process.\nEven when we consider the posterior of the true model, we may observe how point estimate summaries differ from the true value. Depending on how we quantify the distance between the inference \\(\\hat{p}\\) and the truth \\(p^{\\text{true}}\\), different point estimates may be appropriate. If we define success as only when we get the parameter exactly right (using a \"one-hot\" metric), we should report the MAP estimate since it maximizes this posterior probability. However, if we aim to minimize the squared error (\\(\\ell_2\\) metric) of our inference, we should report the expected a posteriori (EAP) value, which provides the least squares estimate over the posterior. Thus even in the idealized scenario where the data is generated by model, our choice of metric over the parameters influences how we should summarize the inference, either with the mode or the mean of the posterior.\nWhile we can optimize our point estimates accordingly, the posterior distribution can often be highly dispersed or even multimodal. This means that, given the data, multiple parameter values may fit equally well. The true parameter could reside at any of these peaks, meaning that no single point estimate can reliably be close to the truth. Many inference problems undergo a transition between a noisy regime where it is not possible to consistently identify the generating parameters to a data-rich regime where it becomes feasible."
  },
  {
    "objectID": "notes/2025/network_structures.html",
    "href": "notes/2025/network_structures.html",
    "title": "Network structures",
    "section": "",
    "text": "Networks are often used to represent complex systems composed of many parts, such as individual people, animals, or atoms. Their study focuses on the nature and structure of interactions between these components. Just as a universe of only non-interacting particles would be a lifeless soup, a collection of people who never interact is boring and unrealistic to study. The network of interactions between the many pieces makes the system “complex.”\nThis note gives examples of emergent structures found in these networks, and the presentation is adapted from my PhD thesis."
  },
  {
    "objectID": "notes/2025/network_structures.html#representing-network-data",
    "href": "notes/2025/network_structures.html#representing-network-data",
    "title": "Network structures",
    "section": "Representing network data",
    "text": "Representing network data\nIn a network (or \"graph\") these components are abstractly represented as nodes (\"vertices\") and the interactions are represented as edges, lines that connect interacting nodes with each other. This simple framework is flexible enough to span a rich variety of applications and data under various interpretations of the nodes and edges.\n\n\n\n\n\n\nFigure 1: Network of interactions observed by Setia  (Setia and Schaik 2007) among \\(n = 6\\) orangutans represented (a) as a graph and (b) as an adjacency matrix \\(\\boldsymbol{A}\\). (c) The node degrees \\(\\boldsymbol{k}\\), the number of interactions of each orangutan.\n\n\n\nAn example network is given in Figure 1a. There the colored nodes represent 6 Indonesian flanged male orangutans who are connected by an edge if a vocalization between them was observed in a study by Setia  (Setia and Schaik 2007). As reflected in the graph, 5 of the 6 orangutans form a clique of nodes that each interact with each other while the remaining ape represented in red interfaced with only the orange and magenta orangutans.\nSuch a network on \\(n\\) nodes is often alternatively represented by a \\(n \\times n\\) adjacency matrix \\(\\boldsymbol{A}\\), whose entry \\(A_{ij}\\) for \\(i,j = 1,...,n\\) is equal to \\(1\\) if nodes \\(i\\) and \\(j\\) interact and \\(0\\) otherwise. Figure 1b shows the orangutan network in this format with the rows and columns labeled by the orangutan they reference. This matrix and its properties naturally convey information about the network. For example the vector \\(\\boldsymbol{k}\\) of row (or column) sums of the adjacency matrix\n\n\\[\\begin{aligned}\n    k_i = \\sum_{j=1}^n A_{ij}, \\qquad i = 1,...,n,\\end{aligned}\\]\n\ngives the degree of each node, the total number of edges connected to it.\nThese degrees of our apes are given in Figure 1c. The red orangutan is the only node with degree 2, while the orange and magenta orangutans have degree 5 as they both connect to every other orangutan. This simple measure of counting connections captures a sense of centrality within the network as certain orangutans appear more disposed to interact than others. This sort of variation in the degrees is typical of real networks, particularly within large data sets. \nThe edges in Figure 1 are bidirectional since they symmetrically represent whether any interaction occurred between a pair of orangutans. The network is therefore an undirected graph and has a symmetric adjacency matrix. Although such undirected graphs are very common, and the focus of much work, many systems possess asymmetric interactions. In fact, in their observations Setia  not only recorded whether a pair interacted but also which orangutan was dominant over the other from their pattern of vocalizations. We can add this layer of information to the network by converting it to a directed graph, where each edge is now represented by an arrow that points from the dominant \"winner\" of the interaction to the submissive \"loser.\"\nThe original network of Fig. 1 is also a simple graph as interactions are binary – two nodes either do or do not connect and no node connects to itself. However some pairs of orangutans interacted more than once over the course of the study, a feature we can represent with a multigraph where nodes can connect with more than one edge and self-edges can connect a node to itself. Although self-edges are not present in the orangutan context, such self-interactions arise in other settings. In a food web of predation among species, for example, cannibalism within a species is represented with a self-edge.\nBy incorporating these generalizations, the network of Figure 2a represents the full detail of the data set of Setia  (Setia and Schaik 2007). Where the original network of Figure 1 represents if a pair of orangutans interact, this new network gives fuller context as to the nature of the interactions. The adjacency matrix in Figure 2b is no longer a symmetric binary matrix as \\(A_{ij}\\) now represents the number of directed edges from node \\(i\\) to node \\(j\\). This asymmetry generates two distinct notions of degree. Figure 2c contains the row sums of the matrix, the out-degrees \\(\\boldsymbol{k}^{\\text{out}}\\), that indicate the number of edges pointing away from a given node, in this case the number of times each orangutan had a dominant interaction. In Figure 2d the column sums of the matrix give the in-degrees \\(\\boldsymbol{k}^{\\text{in}}\\), the count of edges pointing towards each node, the number of times each orangutan had a submissive interaction. The directions of the data and resulting degrees give further insight to the social world of the orangutans. For example the magenta orangutan has \\(k_i^{\\text{out}} = 9, k_i^{\\text{in}} = 0\\) and so was dominant in each of his 9 interactions, suggesting that he holds a prime position in the orangutans’ social hierarchy.\n\n\n\n\n\n\nFigure 2: Directed multigraph of dominance interactions observed among \\(n = 6\\) orangutans (Setia and Schaik 2007) represented (a) as a graph and (b) as an adjacency matrix \\(\\boldsymbol{A}\\). (c) The out-degrees \\(\\boldsymbol{k}^{\\text{out}}\\) and (d) the in-degrees \\(\\boldsymbol{k}^{\\text{in}}\\) are also given, respectively counting the number of times each orangutan exhibits a dominant or submissive interaction.\n\n\n\nFurther adornments of networks are often made to represent further detail. In a trade network each edge could carry a continuous weight that reports trade volume between two countries, an example of a weighted graph represented by a continuous adjacency matrix (Domenico et al. 2015). Edges can also be differentiated by their type, for example friendships in a social network may be associated to different contexts like work, hobbies, or social media forming a multilayer network of connections between people (Cardillo et al. 2013; Kivelä et al. 2014). Higher-order interactions between triplets and larger groups of nodes may be also be represented either as hypergraphs or bipartite networks.\nAlthough adding increasingly descriptive context and metadata to networks helps to paint a fuller picture of any particular application, by stripping systems down to a simple pattern of interactions it is possible to treat them within a unified language of network science and understand similarities and differences between them. In this note we focus on two common structures found in networks across contexts, group and hierarchical structure."
  },
  {
    "objectID": "notes/2025/network_structures.html#group-structure",
    "href": "notes/2025/network_structures.html#group-structure",
    "title": "Network structures",
    "section": "Group structure",
    "text": "Group structure\nThe nodes that make up a network often possess distinct group identities. Animals are distinguished by species, atoms differ by atomic number, and students belong to various clubs and social groups. These group labels may then influence the interactions among the nodes as, for example, students are often friends with others in the same school club. Figure 3 gives four examples of real networks where the color of each node indicates which group it belongs to. In each system these group affiliations guide the pattern of connections as nodes of the same group share the same structural role.\n\n\n\n\n\n\nFigure 3: Four examples of community structure in networks. (a) American football matches among Division IA colleges in the 2000 regular season (Girvan and Newman 2002). Most matches occur within the labeled \"conferences\" that the teams belong to. (b) Network of political books frequently copurchased on Amazon.com near the 2004 U.S. presidential election labeled by political lean. (c) Network of characters in Shakespeare’s Romeo and Juliet where edge thickness indicates frequency of interaction. The house each character belongs to in the work, Montague or Capulet, is highlighted. (d) Bipartite network of plant-pollinator interactions (Bezerra, Machado, and Mello 2009)\n\n\n\nFigure 3a contains a network of \\(616\\) American football matches played among \\(115\\) Division IA colleges in the 2000 regular season (Girvan and Newman 2002). Each year these football teams group into conferences that agree to play a certain number of matches, often 8, against other teams in the same conference. While matches out-of-conference are permitted, they occur more rarely as they are organized on an ad hoc basis from year to year. The conference system thus gives teams an assortative preference to play teams of the same group. Over the course of the season the network of matches then carries a clear signature of the conferences that inform them.\nSimilarly assortative structure is found in Figure 3b, a network of \\(105\\) political books frequently copurchased on Amazon.com near the 2004 U.S. presidential election. Here the books are colored by the political lean of their content, characterized by V. Krebs in unpublished work. Books of the same political bent are typically purchased together, either both liberal or both conservative. Rarely does someone order both a conservative and liberal book in the data set. In this case the observed assortative structure is not imposed by a collective agreement like the football conference system but rather emerges from individual decisions as an effect of systemic political polarization.\nIn networks of social ties, people and animals often organize into tightly knit communities that prefer to interact amongst themselves. Figure 3c contains a (fictional) example of such a network among characters in Shakespeare’s Romeo and Juliet. Character interactions, defined as subsequent appearances in the same scene, form the edges of this multigraph, where the edge thickness reflects the frequency of the interaction. In the play most characters belong to one of two groups, either Romeo’s House of Montague or Juliet’s House of Capulet. Interactions between characters are indeed mostly contained within each house. The narrative, however, revolves around the titular exception to this pattern. \nNot all group structures are assortative in this manner. In fact many network data sets are bipartite, meaning they consist of two groups of nodes that never interact within their own group, only with the opposite group: a fully disassortative structure. Figure 3d gives an example of a bipartite plant-pollinator network. In it 13 species of Brazilian oil-flowers are connected to which of 13 species of pollinators visited them over the course of a study by Berezza  (Bezerra, Machado, and Mello 2009). In this setting the nodes are naturally sorted by their role as either a plant or a pollinator. Being a bipartite network, all interactions occur only between a plant and a pollinator, never directly between two plants or two pollinators.\nIn these examples and beyond networks possess a variety of group structures, distinguished both in the number of groups present and how those groups inform the pattern of interactions. In the four examples of Figure 3, group identities can be gleaned from context outside the network of interactions. However in many contexts only the pattern of connections is known, although nodes may still meaningfully belong to groups that influence that structure.\nIn this setting a key task of network science is community detection, to analyze a network and identify if group structure exists, identify the groups, and characterize their structural relationships. A variety of approaches have been developed for this task (Newman 2006; Dyer and Frieze 1989). The groups inferred from the network can then be interpreted in their own right or compared to known group labels (Jerdee, Kirkley, and Newman 2024)."
  },
  {
    "objectID": "notes/2025/network_structures.html#hierarchy-structure",
    "href": "notes/2025/network_structures.html#hierarchy-structure",
    "title": "Network structures",
    "section": "Hierarchy structure",
    "text": "Hierarchy structure\nAs in our orangutan example, network interactions are often directed. Edges in a directed graph variously run from a source to a target, a winner to a loser, a leader to a follower. Across a system these asymmetries often follow a coherent direction within a hierarchy of the nodes. In a sports context this may be a hierarchy of skill where a more skilled player is likely to prevail over a less skilled opponent. In social settings \"dominance interactions\" tend to be won by the higher status individual. In Figure 4, four examples of such directed networks are given. In each case the hierarchy of the nodes is plotted along the vertical axis.\n\n\n\n\n\n\nFigure 4: Four examples of hierarchy structure in networks. (a) American football matches played in the 2022 Big Ten conference football season (“College Football Records,” n.d.). The hierarchical structure is represented vertically, with better teams located higher. Arrows run from the winner to the loser of a match, colored green if the higher ranked team wins, red if the lower ranked team wins. (b) Consumer preferences between pairs of sodas (Duineveld, Arents, and King 2000). (c) Observed dominance interactions (pecks) among 11 chickens (Masure and Allee 1934). (d) Unreciprocated friendship nominations among 20 high school students (Udry, Bearman, and Harris 1997).\n\n\n\nFigure 4a contains football matches played within the 2022 Big Ten football conference (“College Football Records,” n.d.). Each arrow points from the winner to the loser of a match, information that was suppressed in the undirected Figure 3a. The University of Michigan was undefeated in this (carefully chosen) season as indicated by all arrows pointing away from its node and by its position at the top of the football hierarchy. In this case placement on the hierarchy is meant to reflect strength in the game of football, and we would expect that better teams win more often than lower ranked ones. Particularly we may assume that the outcome of any given match is driven by the difference in skill between the participating teams. Although the teams and fans are generally aware of these relative strengths, there is not a \"true\" hierarchy to refer to like the conference group structure. The positions in the hierarchy must instead be deduced from the observed matches.\nThere are many ways to determine a hierarchy from the match outcomes. Collegiate football formalizes this inference through an annual poll of team coaches to determine the highest ranked teams who may then advance to the post-season playoff bracket. In Figure 4a, the vertical hierarchy is instead arranged in order to maximize the number of times the higher ranked team wins (the 57 green arrows) and minimize the upset victories (the 7 red arrows). The other networks of Figure 4 are likewise arranged to minimize such violations of their hierarchies. \nThe presence of upsets means that the data does not fully respect the hierarchy. One might hope that there is some other ordering of the teams that is fully coherent, lacking any such upsets, but the intransitivity of the results prevents this. In the football example Minnesota beat Nebraska, Nebraska beat Iowa, and Iowa beat Minnesota in a rock-paper-scissors arrangement. One of these three matches would therefore be recorded as an \"upset\" under any ordering of the teams. In fact any ordering of this season will have at least 7 upsets as in the ordering plotted in the figure. This tension between the hierarchy and the realized results is analogous to how not all edges lie within the groups of Figure 3. The degree to which the outcomes conform to a hierarchy gives insight to its strictness.\nMany of the models used to describe hierarchies were initially developed and are often used in the world of sports, yet similar concepts can be applied to understand status and quality in other contexts. Figure 4b shows a hierarchy of 8 different flavored sodas. In this network wins indicate if most assessors surveyed by Duineveld  prefer one soda over another in a paired comparison study (Duineveld, Arents, and King 2000). Again no total ordering of the sodas is possible as the revealed preferences are not strictly transitive, although there are certain sodas that generally fare better in these comparisons. Such surveys and inferences are often used to make sense of A/B testing of various types of products and establish the aggregate consensus of quality and preferences. These methods are similarly used in reinforcement learning to assess and tune large language model outputs based upon human preferences (Christiano et al. 2017).\nTurning to animals, the social hierarchy of a flock of 11 Brown Leghorn chickens is plotted in Figure 4c (Masure and Allee 1934). Here the arrows represent when one chicken pecks another but is not pecked in return, taken as a sign of dominance. The chickens organize into a pecking order where the higher chicken tends to peck the lower chicken on the ladder. This animal behavior research is in fact the source of the phrase \"pecking order\" colloquially used to refer to all sorts of hierarchies. In this flock of 11, the chicken at the bottom of the pecking order is pecked by all above it while there is more competition and ambiguity at the top.\nFigure 4d shows a similar social hierarchy, now of surveyed friendship nominations among 20 students of a small U.S. high school (Udry, Bearman, and Harris 1997). Although we may think of friendship as a two-way street, this reciprocity is not always borne out in survey data. Often student A names student B as their friend but student B does not name student A back. We can interpret this as a \"win\" for student B in the social hierarchy, and that they are likely of higher social status than student A. In the school of this figure a clear hierarchy emerges where higher status students consistently dominate the lower ranked students in this manner.\nEach of the directed network representations in Figure 3 foregoes further context unique to each setting. The football match network, for example, neglects idiosyncratic details like player injuries or home-field advantages that can influence the outcome of any given match. Such events, however, do not have a clear analog within, say, the pecking order of chickens. By reducing the systems down to a directed pattern of \"wins\" and \"losses,\" we can directly compare them to each other. In (Jerdee and Newman 2024) we discuss how this framework enables us to observe how the nature and strength of these hierarchies differ across settings."
  },
  {
    "objectID": "thoughts.html",
    "href": "thoughts.html",
    "title": "Thoughts and Notes",
    "section": "",
    "text": "This is a rough TODO list/notepad for development for the site.\n\n\nText-based ramblings that can cover a range of possible uses. They are also tagged with categores as: - Paper explainer: informal explanation of a paper that we have put out, ideally accompanied by some sort of video explaination - Demo explainer: explanation of some technical details behind the inner workings of a demo - Review: a review of a field or introduction (these are what most of the spin-off posts from my thesis are) - Physics - Information theory - Inference\nSpecial categories for the notes posts are roughly supposed to be: Paper explainer: informal explanation of a paper that we have put out, ideally accompanied by some sort of video explaination Demo explainer: explanation of some technical details behind the inner workings of a demo Review: a review of a field or introduction (these are what most of the spin-off posts from my thesis are)\nOtherwise I imagine that most of the posts will be explanations of some idea that comes up in my research or something I am generally curious about that has the developed demos embedded within the explanations.\n\n\nIn order to run the site locally, you will need to have quarto installed. You can install it by following the instructions at quarto.org/docs/get-started/. To run the site locally, you can use the following command:\nquarto preview\nin the quarto_src directory. This will start a local serve and give a link to view the site in your browser.\nIn order to build and host the site, run quarto render in the quarto_src directory. This will build the site and output it to the docs directory, which is where GitHub Pages will serve the site from. Then push to the main branch of the repository. GitHub Pages will automatically build and host the site from the docs directory.\n\n\n\nTo create a new note, we should start by creating a new Quarto markdown file (.qmd) in the quarto_src/notes/drafts directory. The file should have a title and a date in the front matter, as well as a category. The category should be one of the following: paper-explainer, demo-explainer, or general. The file should also have a description in the front matter, which will be used to generate the summary for the note. We can also generate an initial note using pandoc from say a TeX file as pandoc -s myfile.tex -o myfile.md and then edit the resulting file to add the front matter and any additional content we want to include. We will need to add an assets-folder to the front matter then reference any images using that folder, for example: ?meta:assets-folder/image.png.\npdfs will also render in a strange way, we can convert them either to raster images or another vector format like svg. This can be done using for example inkscape as\n/mnt/c/Program\\ Files/Inkscape/bin/inkscape.exe --export-type=\"svg\" coin-flip-fair.pdf\nWe will also need to manually redo the cross-references. TODO: write a script that automatically adjusts the .md file output by pandoc into the .qmd format with the correct front matter and cross-references.\nOnce the note is ready, we can move it to the quarto_src/notes/YEAR directory."
  },
  {
    "objectID": "thoughts.html#notes",
    "href": "thoughts.html#notes",
    "title": "Thoughts and Notes",
    "section": "",
    "text": "Text-based ramblings that can cover a range of possible uses. They are also tagged with categores as: - Paper explainer: informal explanation of a paper that we have put out, ideally accompanied by some sort of video explaination - Demo explainer: explanation of some technical details behind the inner workings of a demo - Review: a review of a field or introduction (these are what most of the spin-off posts from my thesis are) - Physics - Information theory - Inference\nSpecial categories for the notes posts are roughly supposed to be: Paper explainer: informal explanation of a paper that we have put out, ideally accompanied by some sort of video explaination Demo explainer: explanation of some technical details behind the inner workings of a demo Review: a review of a field or introduction (these are what most of the spin-off posts from my thesis are)\nOtherwise I imagine that most of the posts will be explanations of some idea that comes up in my research or something I am generally curious about that has the developed demos embedded within the explanations.\n\n\nIn order to run the site locally, you will need to have quarto installed. You can install it by following the instructions at quarto.org/docs/get-started/. To run the site locally, you can use the following command:\nquarto preview\nin the quarto_src directory. This will start a local serve and give a link to view the site in your browser.\nIn order to build and host the site, run quarto render in the quarto_src directory. This will build the site and output it to the docs directory, which is where GitHub Pages will serve the site from. Then push to the main branch of the repository. GitHub Pages will automatically build and host the site from the docs directory.\n\n\n\nTo create a new note, we should start by creating a new Quarto markdown file (.qmd) in the quarto_src/notes/drafts directory. The file should have a title and a date in the front matter, as well as a category. The category should be one of the following: paper-explainer, demo-explainer, or general. The file should also have a description in the front matter, which will be used to generate the summary for the note. We can also generate an initial note using pandoc from say a TeX file as pandoc -s myfile.tex -o myfile.md and then edit the resulting file to add the front matter and any additional content we want to include. We will need to add an assets-folder to the front matter then reference any images using that folder, for example: ?meta:assets-folder/image.png.\npdfs will also render in a strange way, we can convert them either to raster images or another vector format like svg. This can be done using for example inkscape as\n/mnt/c/Program\\ Files/Inkscape/bin/inkscape.exe --export-type=\"svg\" coin-flip-fair.pdf\nWe will also need to manually redo the cross-references. TODO: write a script that automatically adjusts the .md file output by pandoc into the .qmd format with the correct front matter and cross-references.\nOnce the note is ready, we can move it to the quarto_src/notes/YEAR directory."
  }
]