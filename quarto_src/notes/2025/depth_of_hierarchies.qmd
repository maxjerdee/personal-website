---
title: "Depth of hierarchies"
description: "Context for *Luck, skill, and depth of competition in games and social hierarchies*"
categories:
  - Inference
  - Paper Explainer
date: 11/3/25
bibliography: ../../assets/LaTeX/references.bib
assets-folder: "../../assets/images/notes/depth_of_hierarchies"
image: ../../assets/images/notes/depth_of_hierarchies/thumbnail.png
# include-in-header:
#     - file: ../../assets/LaTeX/preamble.tex
---

This post gives some context for a [paper](https://www.science.org/doi/10.1126/sciadv.adn2654) written in collaboration with Mark Newman. The presentation here is adapted from my [PhD thesis](../2025/thesis.html).

## Motivation

When players enter a sports tournament, students attend a high school, or chickens are placed in a coop, hierarchies tend to
emerge (along with those who study them). This pecking order can be reflected in chess game
victories, unreciprocated friendships, or chicken pecks. Across these [examples](../2025/network_structures.html#hierarchy-structure), we would like to infer the rankings of the participants and predict the
outcomes of unobserved matches. In this work we introduce a model that allows us to not only find positions within such a hierarchy but also to measure how strict, how unequal that hierarchy is. Across the contexts we consider we find sports are relatively egalitarian as matches could typically go either way, while animal hierarchies follow a much stricter, predictable order. The examples of human social hierarchies we consider, from patterns of friendship to faculty hiring, tend to fall between these extremes. 

## Minimum Violation Ranking

Before describing our full model, we begin with a simpler definition of hierarchies. Many observers
of these systems, especially in sports, may come to their own, often
quite different conclusions about the appropriate ranking of the
competitors. Out of these possibilities, it is useful to have a
consistent benchmark for what constitutes a \"good\" or \"fair\" ranking
given the results.

A natural starting point is to ask that a ranking is consistent with the
observations in the following sense: the favorite, the higher ranked
player, should typically win the match. To analyze a ranking in this
way, we first specify it as vector $\boldsymbol{s}$ where player $i$ is
assigned score $s_i$, and player $i$ is considered better than
player $j$ in the ranking if they have a higher score, $s_i > s_j$. In
this notation, we can then count up the number of times that the
favorite indeed wins as 

:::{style="overflow-x:auto;overflow-y:hidden;"}
$$\begin{align}
    m_{\text{win}}(\boldsymbol{A},\boldsymbol{s}) = \sum_{ij} A_{ij} \mathbf{1} \{s_i > s_j\}
\end{align}$$
:::
where the adjacency matrix entry $A_{ij}$ counts the number of times player $i$ beats player $j$.

In lieu of a known rating of the participants, we can then define a
ranking by maximizing the number of times the favorite indeed wins as


:::{style="overflow-x:auto;overflow-y:hidden;"}
$$\begin{align}
    \boldsymbol{s}* = \text{argmax}_{\boldsymbol{s}} m_{\text{win}}(\boldsymbol{A},\boldsymbol{s}).
\end{align}$$
:::
 This strategy is known as *minimum violation ranking*
since it is equivalent to minimizing the number of upset wins, or
\"violations\" of the ranking.^[Identifying hierarchy as the optimum of an objective is similar to how the modularity is often maximized to identify group structure [@Newman06b].] It is computationally demanding to find
the true optimal ranking in this sense, and heuristic algorithms are
often employed [@CCP23]. In [Figure @fig-MVR]a we use this method to rank the football teams within the 2022 season Big Ten conference [@Football22] based upon the wins and losses amongst them. Although no perfectly consistent ranking is possible, when we vertically arrange the 14 teams according to the minimum violation ranking $m_{\text{win}} = 57$ of the $m = 64$ total matches are won by the favored team (highlighted in green). 

![College football matches played in the 2022 Big Ten conference
football season. (a) Arrows represent that one team beat another, teams
are vertically arranged to minimize the number of upset victories (in
red). (b) Arrows indicate that one team hosts another. Teams are
vertically positioned to minimize ranking violations. (c) The number of
times the favorite \"wins\" in each context, beating or hosting, are
compared against 10,000 simulations where outcomes are sampled as fair
coin
flips.]({{< meta assets-folder >}}/MVR.svg){#fig-MVR width=8.744850000000001in}

We can also use the number of favorite wins $m_{\text{win}}$ as a test statistic
to establish the presence of a hierarchy in a [significance test](../2025/statistical_inference.html#a-fair-coin). As a null hypothesis, we use a
\"fair coin\" model that either team is equally likely to prevail in any
given match regardless of their ranking, each outcome might as well be a flip of a coin. In this null model, the favorite team wins half the time for an average
of $m_{\text{win}} = 32$. However due to random fluctuations the favorites may
happen to accumulate more wins than expected. 

In [Figure @fig-MVR]c we perform 10,000 simulations of the null model, of flipping a coin to decide each match, and find that not once did the favorite win more than $m_{\text{win}} = 47$ matches. Since this is far from the number~$m_{\text{win}} = 57$ we observe in the football hierarchy, we can reject the fair coin model and
conclude that genuine gaps in skill are driving the match outcomes.

In [Figure @fig-MVR]b we
plot another network of the very same matches within the conference, but
now where a \"win\" indicates that one team hosted the other rather than
beat them. Intuitively we might expect this interaction to not be
strongly driven by a hierarchy among the teams, but rather be more akin
to the coin flip model. Yet, if again identify the minimum violations
ranking, we can construct an ordering of the teams where the favorite
wins $m_{\text{win}} = 45$ times. Returning to the significance testing,
in only 8 of the 10,000 simulations did random flips generate more
favorite wins than this mark for a p-value $P < 0.001$. In isolation
this would be seen as very strong evidence of our found hierarchy,
although since this ranking is chosen among $14! = 87178291200$ possible
orderings, the low p-value may not be as impressive as it first appears.

## The Bradley-Terry model

In order to establish that a hierarchy is present in a more intrinsic
manner, and to make predictions (e.g. who will win the next game?) we
turn to generative models that assign a probability to each potential outcome of the matches. Particularly, we consider the
Bradley-Terry model, named after the work of R. Bradley and M. Terry who
described it in 1952 [@BT52], although it was (unknown to them) first
introduced much earlier, by Zermelo in 1929 [@Zermelo29]. In the model,
the probability $p_{ij}$ that node $i$ beats node $j$ is assumed to
depend upon the difference $s_i - s_j$ between their scores.
Specifically, the win probability is taken to be a logistic sigmoid
function of this difference as 

:::{style="overflow-x:auto;overflow-y:hidden;"}
$$\begin{align}
    p_{ij} = \frac{1}{1 + e^{-(s_i - s_j)}}. 
\end{align}$$ {#eq-BT-win-probability}
:::


![Win probability $p_{ij}$ as a function of score difference $s_i - s_j$
in the Bradley-Terry model as in
[Eq. @eq-BT-win-probability].]({{< meta assets-folder >}}/BT-curve.svg){#fig-BT-curve width=8.300983333333333in}

This function, plotted in [Figure @fig-BT-curve], has a number of intuitive properties. If the
two participants are evenly matched, $s_i = s_j$, $p_{ij} = 1/2$ and
each competitor is equally likely to prevail. If node $i$ is
considerably better than node $j$, $s_i \gg s_j$, they are very likely
to win as $p_{ij} \rightarrow 1$. Conversely if they are thoroughly
outmatched and $s_i \ll s_j$, node $i$ is unlikely to win
as $p_{ij} \rightarrow 0$. This *score function* also critically assigns
a behavioral meaning to a particular score differential. If node $i$ is
one \"unit\" above node $j$ in the hierarchy, they will win with
probability 

:::{style="overflow-x:auto;overflow-y:hidden;"}
$$\begin{align}
    p_{ij} = \frac{1}{1 + e^{-1}} \approx 0.731.
\end{align}$$
:::
 This gap then defines the meaning of a \"tier\" or
\"level\" of skill differential, a difference that leads the favorite to
win roughly 70-75% of the time.

Compiling these win probabilities across all of the matches then yields
the Bradley-Terry model likelihood 

:::{style="overflow-x:auto;overflow-y:hidden;"}
$$\begin{align}
    P(\boldsymbol{A}|\boldsymbol{s}) = \prod_{ij} p_{ij}^{A_{ij}} = \prod_{ij} \left(\frac{1}{1 + e^{-(s_i - s_j)}}\right)^{A_{ij}}.
\end{align}$$
:::

In most treatments of the Bradley-Terry model, we then find the values of the scores that maximize this likelihood of generating the data that we observe. These *maximum-likelihood* values, however, are generally prone to [overfitting](../2025/statistical_inference.html#a-biased-coin). If a given team (such as the University of Michigan) finishes undefeated, the model assigns them an infinite maximum-likelihood score, effectively suggesting they always had a 100% chance of winning against any opponent, past or future — an extrapolation that could make even the most ardent fan blush.

## Depth and Bayesian inference

To address this is issue we consider a Bayesian treatment of the Bradley-Terry model. This perspective naturally allows us to not only quantify the uncertainty in our inferences but also measure the nature of these hierarchies. For this we introduce a *prior distribution* that reflects our expectation about the overall distribution of strengths within the hierarchy. Particularly, on each score $s_i$ we introduce an independent Gaussian
prior of width $\beta/\sqrt{2}$ for parameter $\beta > 0$,


:::{style="overflow-x:auto;overflow-y:hidden;"}
$$\begin{align}
    P(\boldsymbol{s}|\beta) = \prod_{i=1}^n \frac{1}{\beta\sqrt{\pi}} e^{-s^2/\beta^2}. \label{eq:P-s-given-beta}
\end{align}$$
:::
 This choice is made so that the distribution of the
*differences* in scores $s_i - s_j$ follow a Gaussian distribution of
width $\beta$. The $\beta$ parameter therefore controls the typical
difference in score between two random players. Given the meaning of one
unit of score difference, $\beta$ counts how many layers of skill or
status are present in the hierarchy between the typical pair. With this
interpretation, we define this parameter in our work as the *depth of competition* [@JN24].

![Example generative process for the Bradley-Terry model of hierarchies.
(a) The depth $\beta$ is first fixed, then the scores $\boldsymbol{s}$ are
sampled from a Gaussian of that width, represented by the vertical
distribution with width $\beta = 3$. The differences of these scores
then inform who wins each match, where the higher score participant is
favored. (b) This same process starting with a depth of $\beta = 1$.
This smaller depth leads to smaller differences in scores and so more
upset wins, colored in
red.]({{< meta assets-folder >}}/BT-generation.svg){#fig-BT-generation width=8.855837083333334in}

Starting with a half-Cauchy prior
$P(\beta)$ over the depth,
[Figure @fig-BT-generation] demonstrates the full generative process
in the Bradley-Terry model.^[We use a Half-cauchy prior $P(\beta) = \frac{8}{\pi(\beta^2 + 16)}$ of width 4, although the form of this single choice does not influence the overall results nearly as much as the prior over the scores which impacts each player individually.]
[Figure @fig-BT-generation]a shows an example where the
depth $\beta = 3$ is relatively high. Since the typical difference
between scores is large, most matches are won by the favorite. In
contrast, [Figure @fig-BT-generation]b illustrates a lower depth of $\beta = 1$
where the now smaller score differences lead to more upsets --
violations of the ranking. From this perspective we also notice that if
the depth is $\beta = 0$ all of the scores must be the same and so all
matches are even, recovering the fair coin null model considered
earlier.

Given a network $\boldsymbol{A}$ we can then take Markov Chain Monte Carlo ([MCMC](../2025/statistical_physics.html#markov-chain-monte-carlo)) samples from the
posterior distribution of potential values for the scores $\boldsymbol{s}$ and the depth $\beta$,


:::{style="overflow-x:auto;overflow-y:hidden;"}
$$\begin{align}
    P(\boldsymbol{s}, \beta|\boldsymbol{A}) = \frac{P(\boldsymbol{A}|\boldsymbol{s})P(\boldsymbol{s}|\beta)P(\beta)}{P(\boldsymbol{A})}.
\end{align}$$
:::
 In [Figure @fig-BT-posteriors] the resulting posterior distributions of
the depth for the football victories and football hosting data sets are
plotted. For the pattern of victories, we can infer a depth
of $\hat{\beta}_{\text{MAP}} \approx 1.9$ within the conference,
indicating that there are roughly 2 levels of play between a random pair
of teams. Particularly, the victories posterior clearly excludes the
case $\beta = 0$, indicating strong evidence for the hierarchy over the
fair coin model. On the other hand, the pattern of hosting among the
teams favors a depth of 0, indicating that hierarchical structure is not
present and that the fair coin model is more appropriate.^[We can also support these conclusions through direct computations of the description length and predictive power of each model for each data set. (See Chapter 1.2.4 of my [PhD thesis](../2025/thesis.html))] 

![Posterior distribution of the depth $\beta$ inferred by the general
Bradley-Terry in the football victories and football hosting data sets.
There is strong evidence of a hierarchical structure among the pattern
of victories, while the pattern of hosting does not exclude the
possibility that outcomes are fully random, highlighted
at $\beta = 0$.]({{< meta assets-folder >}}/BT-posteriors.svg){#fig-BT-posteriors width=8.16855in}

In [our paper](https://www.science.org/doi/10.1126/sciadv.adn2654), we discuss how this model can be further
generalized to model a \"luck\" component of hierarchies where upsets
are possible even between competitors of very different status. This
generalization also includes the minimum violation ranking originally
considered in this section, allowing for a direct comparison of the two
ranking methods. In that chapter we also infer the depths of a variety
of different data sets, ranging from sports and games to human and
animal social hierarchies.
[Figure @fig-depth-comparison] summarizes these results.

![Depths of hierarchies inferred by our model across various
applications. A depth of $\beta = 0$ corresponds to outcomes determined
by fair coin flips while higher values of $\beta$ indicate deeper,
stricter hierarchies. Full posterior distributions of $\beta$ for these
cases are given in [our paper](https://www.science.org/doi/10.1126/sciadv.adn2654).]({{< meta assets-folder >}}/depth-comparison.svg){#fig-depth-comparison width=9.283525216666668in}

The sports and games we consider in
[Figure @fig-depth-comparison] -- highlighted in red -- vary in their
depth yet tend to have a low $\beta$ compared to other contexts. Human
social hierarchies (in green), such as patterns of friendships or of
faculty hiring between universities, have a deeper, steeper hierarchy
than these sports. And more than both these cases, we find that the
animal social hierarchies (in blue) are very strict, with exceedingly
large $\beta$ values in some cases. Although the model does not know the
source of a given network, whether it represents sports matches, human
or animal interactions, it consistently categories the examples by
measuring the depth of each context. With this ability to measure
properties like the inequality in a hierarchy in hand, we can start to
speculate and investigate what factors lead to these contrasting
effects. For example, sports leagues are designed to be competitive for
entertainment value, whereas social hierarchies are subject to no such
incentives. By considering all of these applications within a unified
model we can draw comparisons between them.

By simply extending this near century-old model using Bayesian techniques, we have been able to both better understand positions within hierarchies and to measure the depth of these hierarchies: how those differences in rank translate to differences in behaviors and outcomes depending on the setting.
